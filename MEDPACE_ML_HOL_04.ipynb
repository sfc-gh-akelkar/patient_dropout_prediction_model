{
 "metadata": {
  "kernelspec": {
   "display_name": "Python37 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "lastEditStatus": {
   "notebookId": "yzmcxk2nk2r2xusctfml",
   "authorId": "184210807227",
   "authorName": "AKELKAR",
   "authorEmail": "adwait.kelkar@snowflake.com",
   "sessionId": "d1c75ad2-f427-4815-b4a2-1ba46a52c1d9",
   "lastEditTime": 1760215901205
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e79ae8e5-aec2-4276-9443-074c3a614142",
   "metadata": {
    "collapsed": false,
    "name": "INTRO_MD"
   },
   "source": [
    "# ❄️ End-to-end ML Demo ❄️\n",
    "\n",
    "In this worfklow we will work through the following elements of a typical tabular machine learning pipeline.\n",
    "\n",
    "### 1. Use Feature Store to track engineered features\n",
    "* Store feature defintions in feature store for reproducible computation of ML features\n",
    "      \n",
    "### 2. Train two Models using the Snowflake ML APIs\n",
    "* Baseline XGboost\n",
    "* XGboost with optimal hyper-parameters identified via Snowflake ML distributed HPO methods\n",
    "\n",
    "### 3. Register both models in Snowflake model registry\n",
    "* Explore model registry capabilities such as **metadata tracking, inference, and explainability**\n",
    "* Compare model metrics on train/test set to identify any issues of model performance or overfitting\n",
    "* Tag the best performing model version as 'default' version\n",
    "### 4. Set up Model Monitor to track 1 year of predicted and actual loan repayments\n",
    "* **Compute performance metrics** such a F1, Precision, Recall\n",
    "* **Inspect model drift** (i.e. how much has the average predicted repayment rate changed day-to-day)\n",
    "* **Compare models** side-by-side to understand which model should be used in production\n",
    "* Identify and understand **data issues**\n",
    "\n",
    "### 5. Track data and model lineage throughout\n",
    "* View and understand\n",
    "  * The **origin of the data** used for computed features\n",
    "  * The **data used** for model training\n",
    "  * The **available model versions** being monitored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2512cb5-15ae-40b2-84c7-8a44a9979670",
   "metadata": {
    "language": "python",
    "name": "pip_installs",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#!pip install shap snowflake-ml-python==1.11.0"
  },
  {
   "cell_type": "markdown",
   "id": "7d3fdf8b-d639-4f76-8ee7-ec0e05372e0a",
   "metadata": {
    "name": "md_import",
    "collapsed": false
   },
   "source": "\n### Import Libraries for End-to-End ML Workflow\n\nImport all required libraries for data processing, model training, MLOps, and visualization in Snowflake.\n\n**Categories:**\n\n**Standard ML Libraries:**\n- `pandas`, `numpy`: Data manipulation\n- `sklearn`, `xgboost`: Model training and metrics\n- `shap`: Model explainability (Shapley values)\n- `streamlit`: Interactive visualizations\n\n**Snowflake ML (MLOps):**\n- `Registry`: Model versioning and deployment\n- `tune`, `get_tuner_context`: Distributed hyperparameter optimization\n- `FeatureStore`, `FeatureView`, `Entity`: Feature engineering and management\n\n**Snowpark (Distributed Processing):**\n- `DataFrame`: Lazy DataFrames executing in Snowflake warehouse\n- `functions`: Feature engineering (month, dayofweek, avg, sql_expr, etc.)\n- `Window`: Aggregations across row groups (county averages, rolling windows)\n- `get_active_session`: Snowflake connection and context\n\n**Together:** Enable complete ML workflow from data to production without moving data outside Snowflake.\n"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "imports_and_session",
    "resultHeight": 84
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport sklearn\nimport math\nimport pickle\nimport shap\nfrom datetime import datetime\nimport streamlit as st\nfrom xgboost import XGBClassifier\n\n# Snowpark ML\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.modeling.tune import get_tuner_context\nfrom snowflake.ml.modeling import tune\nfrom entities import search_algorithm\n\n#Snowflake feature store\nfrom snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\n\n# Snowpark session\nfrom snowflake.snowpark import DataFrame\nfrom snowflake.snowpark.functions import col, to_timestamp, min, max, month, dayofweek, dayofyear, avg, date_add, sql_expr\nfrom snowflake.snowpark.types import IntegerType\nfrom snowflake.snowpark import Window\n\n# warning suppresion\nimport warnings; warnings.simplefilter('ignore')\n\n#setup snowpark session\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\nsession\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78265b8-8baa-4136-a32a-32f3f620949d",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "set_version_num_and_vars"
   },
   "outputs": [],
   "source": "#Update this VERSION_NUM to version your features, models etc!\nVERSION_NUM = '0'\nDB = \"SANDBOX\" \nSCHEMA = \"MEDPACE_HOL\" \nCOMPUTE_WAREHOUSE = \"APP_WH\" "
  },
  {
   "cell_type": "code",
   "id": "ec4eb53f-f596-4bb4-ad92-d6fcc1d40a05",
   "metadata": {
    "language": "sql",
    "name": "set_context",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Using Warehouse, Database, and Schema created during Setup\nUSE WAREHOUSE APP_WH;\n\nUSE DATABASE SANDBOX;\n\nUSE SCHEMA MEDPACE_HOL;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9ecb094-35c4-4a86-be2f-e702bee618b1",
   "metadata": {
    "language": "sql",
    "name": "create_new_stage",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE STAGE IF NOT EXISTS MEDPACE_HOL.MORTGAGE_LENDING_DEMO_STAGE \n\tDIRECTORY = ( ENABLE = true );",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dae9ec12-0754-4ad5-92da-f9bbfc56020e",
   "metadata": {
    "name": "md_file_format",
    "collapsed": false
   },
   "source": "### Create a file format to ingest CSVs while preserving headers."
  },
  {
   "cell_type": "code",
   "id": "05f52267-4621-48de-8fee-0c680f41b7e2",
   "metadata": {
    "language": "sql",
    "name": "create_ff_w_headers",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- create csv format\nCREATE FILE FORMAT IF NOT EXISTS SANDBOX.MEDPACE_HOL.CSV_W_HEADERS \n   PARSE_HEADER = true\n    TYPE = 'CSV';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8900d1d-a1f2-419b-ae7e-b194f268d904",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "create_table_from_csv",
    "resultHeight": 223
   },
   "outputs": [],
   "source": "    CREATE OR REPLACE TABLE MORTGAGE_LENDING_DEMO_DATA\n        USING TEMPLATE (\n            SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))\n            FROM TABLE(\n                INFER_SCHEMA(\n                    LOCATION => '@sandbox.medpace_hol.mortgage_lending_demo_stage/MORTGAGE_LENDING_DEMO_DATA.csv',\n                    FILE_FORMAT => 'CSV_W_HEADERS'\n                )\n            )\n        );"
  },
  {
   "cell_type": "code",
   "id": "a1304c95-3397-49af-a0ff-b784c683b2a7",
   "metadata": {
    "language": "sql",
    "name": "desc_table",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "desc table MORTGAGE_LENDING_DEMO_DATA;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e4ee279b-cbe4-4dec-9db4-326b63a2fdc1",
   "metadata": {
    "language": "sql",
    "name": "load_stage_file",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "COPY INTO MORTGAGE_LENDING_DEMO_DATA\nFROM @sandbox.medpace_hol.mortgage_lending_demo_stage/MORTGAGE_LENDING_DEMO_DATA.csv\nFILE_FORMAT = (FORMAT_NAME = 'CSV_W_HEADERS')\nMATCH_BY_COLUMN_NAME = CASE_INSENSITIVE",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "60938b6f-bda7-4783-ae44-547bd34d98de",
   "metadata": {
    "collapsed": false,
    "name": "md1"
   },
   "source": [
    "## Observe Snowflake Snowpark table properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6654de7-6407-4ffe-a214-fd66078397ef",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "see_timespan",
    "resultHeight": 111
   },
   "outputs": [],
   "source": "df = session.table(\"MORTGAGE_LENDING_DEMO_DATA\")\ndf.select(min('TS'), max('TS'))"
  },
  {
   "cell_type": "markdown",
   "id": "6ef7040a-4a50-4977-8ef0-f9de0ba2e9de",
   "metadata": {
    "name": "md_time_adjustment",
    "collapsed": false
   },
   "source": "\n### Calculate Timestamp Adjustment\n\nShift historical timestamps to make the dataset current for realistic model monitoring demos.\n\n**Process:**\n\n1. **Get current date:** `datetime.now()`\n2. **Find dataset max date:** Extract latest timestamp from `TS` column\n3. **Calculate difference:** `timedelta = current_time - df_max_time`\n4. **Preview adjusted range:** Show min/max after adding `timedelta.days - 1` to all timestamps\n\n**Example:**\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a38cc-c479-4839-b0ae-9e5cb3e0facb",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "find_timedelta",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get current date and time\n",
    "current_time = datetime.now()\n",
    "df_max_time = datetime.strptime(str(df.select(max(\"TS\")).collect()[0][0]), \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "#Find delta between latest existing timestamp and today's date\n",
    "timedelta = current_time- df_max_time\n",
    "\n",
    "#Update timestamps to represent last ~1 year from today's date\n",
    "df.select(min(date_add(to_timestamp(\"TS\"), timedelta.days-1)), max(date_add(to_timestamp(\"TS\"), timedelta.days-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa46c7d-519b-422c-8932-9b031fc6b4bd",
   "metadata": {
    "collapsed": false,
    "name": "feat_eng_md"
   },
   "source": [
    "## Feature Engineering with Snowpark APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deca1963-fc62-4d5c-b8e6-fda2fd2761a4",
   "metadata": {
    "name": "md_define_features",
    "collapsed": false
   },
   "source": "\n### Feature Engineering with Snowpark\n\nCreate 10 ML features using a dictionary-based approach for clean, maintainable code.\n\n**Feature Categories:**\n\n**1. Temporal (4):** Adjusted timestamp + seasonality (MONTH, DAY_OF_YEAR, DOTW)\n- Capture seasonal patterns and weekly trends\n\n**2. Financial (3):** Convert units + affordability ratio\n- `LOAN_AMOUNT`, `INCOME` (thousands → dollars)\n- `INCOME_LOAN_RATIO` = affordability metric (higher = lower risk)\n\n**3. Geographic Context (2):** Window functions for county-level comparison\n- `MEAN_COUNTY_INCOME`: Average per county\n- `HIGH_INCOME_FLAG`: Binary (above/below county average)\n- Same income means different things in different areas!\n\n**4. Time-Series (1):** Rolling 30-day average using SQL window\n- `AVG_THIRTY_DAY_LOAN_AMOUNT`: Market trend indicator\n- Detects hot/cooling markets by county\n\n**Key Techniques:**\n- Window functions: `avg(...).over(Window.partition_by(...))`\n- Rolling windows: `RANGE BETWEEN INTERVAL '30 DAYS' PRECEDING...`\n- Type conversions: `(boolean).astype(IntegerType())`\n- Derived ratios: Division of existing features\n\n**Result:** All transformations execute in Snowflake warehouse (no data movement), creating 10 engineered features in one operation via `df.with_columns()`.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b355c0c4-9dc6-4faf-86b7-24d8d559e453",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "define_features",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "#Create a dict with keys for feature names and values containing transform code\n",
    "\n",
    "feature_eng_dict = dict()\n",
    "\n",
    "#Timstamp features\n",
    "feature_eng_dict[\"TIMESTAMP\"] = date_add(to_timestamp(\"TS\"), timedelta.days-1)\n",
    "feature_eng_dict[\"MONTH\"] = month(\"TIMESTAMP\")\n",
    "feature_eng_dict[\"DAY_OF_YEAR\"] = dayofyear(\"TIMESTAMP\") \n",
    "feature_eng_dict[\"DOTW\"] = dayofweek(\"TIMESTAMP\")\n",
    "\n",
    "# df= df.with_columns(feature_eng_dict.keys(), feature_eng_dict.values())\n",
    "\n",
    "#Income and loan features\n",
    "feature_eng_dict[\"LOAN_AMOUNT\"] = col(\"LOAN_AMOUNT_000s\")*1000\n",
    "feature_eng_dict[\"INCOME\"] = col(\"APPLICANT_INCOME_000s\")*1000\n",
    "feature_eng_dict[\"INCOME_LOAN_RATIO\"] = col(\"INCOME\")/col(\"LOAN_AMOUNT\")\n",
    "\n",
    "county_window_spec = Window.partition_by(\"COUNTY_NAME\")\n",
    "feature_eng_dict[\"MEAN_COUNTY_INCOME\"] = avg(\"INCOME\").over(county_window_spec)\n",
    "feature_eng_dict[\"HIGH_INCOME_FLAG\"] = (col(\"INCOME\")>col(\"MEAN_COUNTY_INCOME\")).astype(IntegerType())\n",
    "\n",
    "feature_eng_dict[\"AVG_THIRTY_DAY_LOAN_AMOUNT\"] =  sql_expr(\"\"\"AVG(LOAN_AMOUNT) OVER (PARTITION BY COUNTY_NAME ORDER BY TIMESTAMP  \n",
    "                                                            RANGE BETWEEN INTERVAL '30 DAYS' PRECEDING AND CURRENT ROW)\"\"\")\n",
    "\n",
    "df = df.with_columns(feature_eng_dict.keys(), feature_eng_dict.values())\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4a522b-aeec-421f-9bf0-0996ed61a4e6",
   "metadata": {
    "name": "md_df_explain",
    "collapsed": false
   },
   "source": "### View Snowpark Query Execution Plan\n\nDisplay the SQL execution plan that Snowflake will use to compute the DataFrame with all engineered features.\n\n---\n\n#### **What `df.explain()` Does:**\n\nShows the **logical and physical query plan** that Snowpark generates from all DataFrame operations, including:\n- Feature transformations\n- Window functions\n- Column additions\n- Data sources\n\n**Purpose:**\n- 🔍 **Understand query structure**: See how Snowpark translates Python to SQL\n- ⚡ **Optimize performance**: Identify expensive operations\n- 📊 **Debug issues**: Understand execution flow\n- 🎓 **Learn Snowpark**: See the generated SQL behind the scenes\n\n---\n\n#### **What You'll See:**\n\nThe output shows a **tree-like structure** of operations that will be executed in Snowflake:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c4ead8-25ac-46cc-9bd9-17eac2f796d5",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "df_explain",
    "resultHeight": 312
   },
   "outputs": [],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d7645e-e0ac-4539-b132-54ce53431402",
   "metadata": {
    "collapsed": false,
    "name": "feature_store_markdown"
   },
   "source": "## Create a Snowflake Feature Store\n\n### Snowflake Feature Store\n\nCreate or connect to a Feature Store for centralized ML feature management with automatic refresh and lineage tracking.\n\n---\n\n#### **What is a Feature Store?**\n\nA centralized system for managing ML features throughout their lifecycle:\n- **Store** feature definitions (transformation logic)\n- **Refresh** features automatically from source data\n- **Track** lineage from raw data → features → models\n- **Share** features across multiple models and teams\n- **Version** features for reproducibility\n\n**Think of it as:** GitHub for ML features - centralized, versioned, and reusable.\n\n---\n\n#### **Key Architecture Insight:**\n\nIn Snowflake, a **Feature Store IS a schema**:\n\n**Benefits of native integration:**\n- ✅ No external system required\n- ✅ Leverage existing Snowflake governance (RBAC, access control)\n- ✅ Query features directly with SQL\n- ✅ Automatic integration with Model Registry and ML Lineage\n\n---\n\n#### **Parameters Explained:**\n\n**`session`**: Active Snowflake connection for executing operations\n\n**`database` + `name`**: Location where Feature Store lives (schema within database)\n\n**`default_warehouse`**: Compute cluster for feature operations\n- Powers automatic feature refresh\n- Executes feature transformations\n- Serves features for training/inference\n\n**`creation_mode=CREATE_IF_NOT_EXIST`**: Idempotent behavior\n- **First run**: Creates new Feature Store schema\n- **Subsequent runs**: Connects to existing Feature Store\n- **Result**: Safe to run notebook multiple times without errors\n\n---\n\n#### **What Gets Created:**\n\nBehind the scenes, Snowflake:\n1. Creates (or connects to) a schema as the Feature Store\n2. Initializes metadata structures for tracking entities and feature views\n3. Links Feature Store to the specified warehouse for compute\n\n**Ready for next steps:**\n- Register Entities (e.g., LOAN_ID)\n- Create Feature Views (with automatic refresh schedules)\n- Generate datasets for model training\n\n---\n\n#### **Why Feature Store Matters:**\n\n**Without Feature Store:**\n- ❌ Feature definitions scattered across notebooks\n- ❌ Manual feature refresh (stale data)\n- ❌ Duplicate feature code across projects\n- ❌ No lineage tracking (where did this feature come from?)\n- ❌ Hard to reuse features across models\n\n**With Feature Store:**\n- ✅ Centralized feature repository\n- ✅ Automatic refresh (features stay current)\n- ✅ Reuse features across multiple models\n- ✅ Complete lineage (data → features → models)\n- ✅ Feature documentation and discovery\n- ✅ Consistent feature computation (training = inference)\n\n---\n\n#### **Next Steps:**\n\nAfter initializing the Feature Store, we'll:\n1. **Define Entities** (e.g., LOAN_ID) - what features describe\n2. **Create Feature Views** - collections of features with refresh schedules\n3. **Generate Datasets** - combine features for model training\n4. **Track Lineage** - automatic lineage to models in Model Registry"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abacdc71-9f2c-419f-8d50-3e8f89be367f",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "define_feature_store",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "fs = FeatureStore(\n",
    "    session=session, \n",
    "    database=DB, \n",
    "    name=SCHEMA, \n",
    "    default_warehouse=COMPUTE_WAREHOUSE,\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67480d6a-183f-4373-aaa8-d3ed8e80e11d",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "list_entities",
    "resultHeight": 111
   },
   "outputs": [],
   "source": [
    "fs.list_entities()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d915406f-e52d-4baf-9f6c-b9e0e8d53e6e",
   "metadata": {
    "collapsed": false,
    "name": "FS_CONFIG_MD"
   },
   "source": "### Register Entity in Feature Store\n\nThis cell creates and registers an **Entity** using idempotent logic to ensure safe re-execution.\n\n---\n\n#### **What is an Entity?**\n\nAn **Entity** represents a **business object** around which features are organized.\n\n- **Entity**: `LOAN_ENTITY`\n- **Represents**: Individual mortgage loans  \n- **Identified by**: `LOAN_ID` (the join key)\n\n**Think of it as:** \"What are we making predictions about?\"  \n→ In this case: Each **LOAN** has features (income, loan amount, location, etc.)  \n→ We want to predict loan approval for each **LOAN**  \n→ Therefore: **LOAN** is our Entity\n\n---\n\n#### **🔑 Key Components**\n\n**`name = \"LOAN_ENTITY\"`**\n- Unique identifier for this entity in the Feature Store\n- Used to reference this entity when creating Feature Views\n\n**`join_keys = [\"LOAN_ID\"]`**\n- Column(s) that uniquely identify each instance of this entity\n- Used to join features together when generating training datasets\n\n**`desc = \"Features defined on a per loan level\"`**\n- Documentation/metadata describing what this entity represents\n\n---\n\n#### **⚙️ Why Try/Except Pattern?**\n\n**Purpose: Idempotency** - Running this cell multiple times produces the same result without errors.\n\n| Run | Behavior | Output |\n|-----|----------|--------|\n| 1st | Entity doesn't exist → Create & register | `\"Registered new entity\"` |\n| 2nd+ | Entity exists → Retrieved successfully | `\"Retrieved existing entity\"` |\n\n**Benefits:**\n- ✅ No duplicates - Won't register the same entity twice\n- ✅ No errors - Won't crash if entity already exists  \n- ✅ Notebook re-runability - Safe to execute multiple times\n- ✅ Development friendly - Can iterate without manual cleanup\n\n---\n\n#### **After Execution**\n\nThe Feature Store now contains:\nFeature Store\n\n└─> Entities\n\n└─> LOAN_ENTITY\n\n    ├─ Join Keys: [LOAN_ID]\n\n└─ Description: \"Features defined on a per loan level\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d6d39-7819-4825-8729-a3f19ca5cdf7",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "load_or_register_entity",
    "resultHeight": 38
   },
   "outputs": [],
   "source": [
    "#First try to retrieve an existing entity definition, if not define a new one and register\n",
    "try:\n",
    "    #retrieve existing entity\n",
    "    loan_id_entity = fs.get_entity('LOAN_ENTITY') \n",
    "    print('Retrieved existing entity')\n",
    "except:\n",
    "#define new entity\n",
    "    loan_id_entity = Entity(\n",
    "        name = \"LOAN_ENTITY\",\n",
    "        join_keys = [\"LOAN_ID\"],\n",
    "        desc = \"Features defined on a per loan level\")\n",
    "    #register\n",
    "    fs.register_entity(loan_id_entity)\n",
    "    print(\"Registered new entity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6dce43-9630-4ebe-9ecf-bb1c84a8ce78",
   "metadata": {
    "name": "md_create_feature_df",
    "collapsed": false
   },
   "source": "### Prepare Feature DataFrame for Feature View\n\nCreate a clean DataFrame containing only the columns needed for the Feature View: entity join key, timestamp, and engineered features.\n\n---\n\n#### **What This Does:**\n\n```python\nfeature_df = df.select([\"LOAN_ID\"] + list(feature_eng_dict.keys()))\n```\n\n**Selects:**\n- **`LOAN_ID`**: Entity join key (required)\n- **All engineered features** from `feature_eng_dict`:\n  - `TIMESTAMP` (required for point-in-time joins)\n  - `MONTH`, `DAY_OF_YEAR`, `DOTW` (temporal features)\n  - `LOAN_AMOUNT`, `INCOME`, `INCOME_LOAN_RATIO` (financial features)\n  - `MEAN_COUNTY_INCOME`, `HIGH_INCOME_FLAG` (geographic features)\n  - `AVG_THIRTY_DAY_LOAN_AMOUNT` (time-series feature)\n\n**Excludes:**\n- ❌ Raw columns (e.g., `LOAN_AMOUNT_000s`, `APPLICANT_INCOME_000s`)\n- ❌ Target variable (stored separately, not in Feature View)\n- ❌ Unnecessary metadata\n\n---\n\n#### **Why Create This Subset?**\n\n**Feature Store requires:**\n1. ✅ Entity join key(s) - `LOAN_ID`\n2. ✅ Timestamp column - `TIMESTAMP` (for point-in-time correctness)\n3. ✅ Feature columns - All engineered features ready for ML\n\nThis DataFrame provides exactly those components for Feature View registration.\n\n---\n\n#### **Preview Output:**\n\n`.show(5)` displays first 5 rows to verify:\n- ✅ All expected columns are present\n- ✅ Feature values look correct\n- ✅ Data is ready for Feature View definition"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820463f-0ea7-43ea-a500-9b034011887d",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "create_feature_df",
    "resultHeight": 217
   },
   "outputs": [],
   "source": [
    "#Create a dataframe with just the ID, timestamp, and engineered features. We will use this to define our feature view\n",
    "feature_df = df.select([\"LOAN_ID\"]+list(feature_eng_dict.keys()))\n",
    "feature_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf84fe3-4120-4092-b43d-8873da57d461",
   "metadata": {
    "collapsed": false,
    "name": "FS_MD"
   },
   "source": "### Define and Register Feature View\n\nCreate a Feature View object that wraps the feature DataFrame with metadata, documentation, and configuration, then register it permanently in the Feature Store.\n\n---\n\n#### **Step 1: Define Feature View**\n\n```python\nloan_fv = FeatureView(\n    name=\"Mortgage_Feature_View\",\n    entities=[loan_id_entity],\n    feature_df=feature_df,\n    timestamp_col=\"TIMESTAMP\",\n    refresh_freq=\"1 day\"\n)\n```\n\n**Key Parameters:**\n- **`name`**: Unique identifier for this Feature View\n- **`entities`**: Links to `LOAN_ENTITY` (defines join key: `LOAN_ID`)\n- **`feature_df`**: Uses the DataFrame from previous cell as data source\n- **`timestamp_col`**: Enables point-in-time correctness for training data generation\n- **`refresh_freq`**: Automatically refreshes features daily\n\n---\n\n#### **Step 2: Add Feature Descriptions**\n\n```python\nloan_fv = loan_fv.attach_feature_desc({...})\n```\n\n**Purpose:**\n- 📝 Documents each feature for team collaboration\n- 🔍 Improves Feature Store discoverability\n- 📊 Enhances governance and lineage tracking\n\n---\n\n#### **Step 3: Register in Feature Store**\n\n```python\nloan_fv = fs.register_feature_view(loan_fv, version=VERSION_NUM, overwrite=True)\n```\n\n**What happens:**\n- ✅ Persists Feature View definition permanently\n- ✅ Creates versioned backend objects in Snowflake\n- ✅ Enables reuse across notebooks, users, and models\n- ✅ Sets up automatic daily refresh schedule\n- ✅ Makes features discoverable and accessible organization-wide\n\n**`overwrite=True`**: Ensures idempotency - safe to re-run without errors\n\n---\n\n#### **Result:**\n\nFeatures are now registered, versioned, documented, and ready for:\n- 🎯 Training dataset generation\n- 🔄 Automatic refresh and maintenance\n- 🤝 Team-wide collaboration\n- 📈 Model training and inference"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b53364f-90c4-45b4-94ee-b2fde6f93475",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "feature_view_creation",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "#define and register feature view\n",
    "loan_fv = FeatureView(\n",
    "    name=\"Mortgage_Feature_View\",\n",
    "    entities=[loan_id_entity],\n",
    "    feature_df=feature_df,\n",
    "    timestamp_col=\"TIMESTAMP\",\n",
    "    refresh_freq=\"1 day\")\n",
    "\n",
    "#add feature level descriptions\n",
    "\n",
    "loan_fv = loan_fv.attach_feature_desc(\n",
    "    {\n",
    "        \"MONTH\": \"Month of loan\",\n",
    "        \"DAY_OF_YEAR\": \"Day of calendar year of loan\",\n",
    "        \"DOTW\": \"Day of the week of loan\",\n",
    "        \"LOAN_AMOUNT\": \"Loan amount in $USD\",\n",
    "        \"INCOME\": \"Household income in $USD\",\n",
    "        \"INCOME_LOAN_RATIO\": \"Ratio of LOAN_AMOUNT/INCOME\",\n",
    "        \"MEAN_COUNTY_INCOME\": \"Average household income aggregated at county level\",\n",
    "        \"HIGH_INCOME_FLAG\": \"Binary flag to indicate whether household income is higher than MEAN_COUNTY_INCOME\",\n",
    "        \"AVG_THIRTY_DAY_LOAN_AMOUNT\": \"Rolling 30 day average of LOAN_AMOUNT\"\n",
    "    }\n",
    ")\n",
    "\n",
    "loan_fv = fs.register_feature_view(loan_fv, version=VERSION_NUM, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c3225b-b936-4aa7-81f2-27bbaeee1c0f",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "show_feature_views",
    "resultHeight": 111
   },
   "outputs": [],
   "source": [
    "fs.list_feature_views()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee1d9e3-22f7-45c2-8060-559510eb1923",
   "metadata": {
    "name": "md_feature_store_link",
    "collapsed": false
   },
   "source": "### Generate Feature Store UI Link\n\nDynamically construct a clickable URL to inspect the newly registered Feature View in Snowflake's Feature Store UI (Snowsight).\n\n---\n\n#### **What This Does:**\n\n```python\norg_name = session.sql('SELECT CURRENT_ORGANIZATION_NAME()').collect()[0][0]\naccount_name = session.sql('SELECT CURRENT_ACCOUNT_NAME()').collect()[0][0]\n```\n\n**Retrieves current Snowflake context:**\n- **`org_name`**: Your Snowflake organization name\n- **`account_name`**: Your Snowflake account name\n\n**Constructs URL:**\n```python\nst.write(f'https://app.snowflake.com/{org_name}/{account_name}/#/features/database/{DB}/store/{SCHEMA}')\n```\n\nBuilds a direct link to the Feature Store UI for this specific database and schema.\n\n---\n\n#### **Why This is Useful:**\n\n**Click the link to explore:**\n- 🔍 **Feature View details** (name, version, entity, refresh schedule)\n- 📊 **Feature catalog** (all features with descriptions)\n- 📈 **Lineage tracking** (data sources and dependencies)\n- 🔄 **Refresh status** (last refresh, next scheduled refresh)\n- 🤝 **Team collaboration** (discoverable by other users)\n\n---\n\n#### **Why Dynamic Construction?**\n\n✅ **Portable** - Works across different Snowflake accounts  \n✅ **Environment-agnostic** - Adapts to dev, test, prod  \n✅ **No hardcoding** - Automatically uses current context  \n✅ **Shareable** - Notebook works for all team members\n\n---\n\n**Click the generated link to visually inspect your Feature View in Snowsight!** 🚀"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7a1aae-0bd2-4aad-b9ed-3347fc56b6ea",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "create_feature_store_link",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create link to feature store UI to inspect newly created feature view!\n",
    "org_name = session.sql('SELECT CURRENT_ORGANIZATION_NAME()').collect()[0][0]\n",
    "account_name = session.sql('SELECT CURRENT_ACCOUNT_NAME()').collect()[0][0]\n",
    "\n",
    "st.write(f'https://app.snowflake.com/{org_name}/{account_name}/#/features/database/{DB}/store/{SCHEMA}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ff67f-bb04-40cb-8c14-11b5ebb2917d",
   "metadata": {
    "collapsed": false,
    "name": "FV_MD"
   },
   "source": "### Generate Dataset from Feature Store\n\nCreate a complete dataset by joining the spine DataFrame with registered Feature Views using point-in-time correctness.\n\n---\n\n#### **What This Does:**\n\n```python\nds = fs.generate_dataset(\n    name=f\"MORTGAGE_DATASET_EXTENDED_FEATURES_{VERSION_NUM}\",\n    spine_df=df.select(\"LOAN_ID\", \"TIMESTAMP\", \"LOAN_PURPOSE_NAME\", \"MORTGAGERESPONSE\"),\n    features=[loan_fv],\n    spine_timestamp_col=\"TIMESTAMP\",\n    spine_label_cols=[\"MORTGAGERESPONSE\"]\n)\n```\n\n---\n\n#### **Key Parameters:**\n\n**`name`**: Unique identifier for this dataset (versioned with `VERSION_NUM`)\n\n**`spine_df`**: The \"backbone\" DataFrame containing:\n- **`LOAN_ID`**: Join key to match with Feature View\n- **`TIMESTAMP`**: For point-in-time feature lookups\n- **`LOAN_PURPOSE_NAME`**: Additional contextual feature\n- **`MORTGAGERESPONSE`**: Target variable (label)\n\n**`features`**: List of Feature Views to join (in this case: `[loan_fv]`)\n\n**`spine_timestamp_col`**: Column for point-in-time correctness\n\n**`spine_label_cols`**: Target variable(s) to include in final dataset\n\n---\n\n#### **🎯 What Happens Under the Hood:**\n\n1. **Point-in-Time Join**: For each row in `spine_df`, fetch features from `loan_fv` as they existed at that `TIMESTAMP`\n2. **Prevents Data Leakage**: Ensures features reflect only information available at that point in time\n3. **Combines Data**: Joins spine columns + Feature View features + labels\n4. **Registers Dataset**: Saves as a reusable, versioned dataset in Feature Store\n\n---\n\n#### **📊 Result:**\n\nA complete ML-ready dataset containing:\n- ✅ Entity keys (`LOAN_ID`)\n- ✅ Timestamps (`TIMESTAMP`)\n- ✅ Additional spine features (`LOAN_PURPOSE_NAME`)\n- ✅ All engineered features from Feature View (MONTH, INCOME, INCOME_LOAN_RATIO, etc.)\n- ✅ Target variable (`MORTGAGERESPONSE`)\n\n**Can be used for:** Model training, testing, validation, inference, or exploratory analysis\n\n---\n\n#### **💡 Why Use Feature Store for Dataset Generation?**\n\n✅ **Point-in-time correctness** - No data leakage  \n✅ **Feature reuse** - Consistent features across use cases  \n✅ **Version control** - Reproducible datasets  \n✅ **Lineage tracking** - Full visibility into data sources  \n✅ **Consistency** - Same feature definitions everywhere"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535efc80-e4fc-41c5-98eb-5b5450bcf199",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "generate_dataset",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "ds = fs.generate_dataset(\n",
    "    name=f\"MORTGAGE_DATASET_EXTENDED_FEATURES_{VERSION_NUM}\",\n",
    "    spine_df=df.select(\"LOAN_ID\", \"TIMESTAMP\", \"LOAN_PURPOSE_NAME\",\"MORTGAGERESPONSE\"), #only need the features used to fetch rest of feature view\n",
    "    features=[loan_fv],\n",
    "    spine_timestamp_col=\"TIMESTAMP\",\n",
    "    spine_label_cols=[\"MORTGAGERESPONSE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b0544-9a4b-4b63-9043-6705b20b776a",
   "metadata": {
    "name": "md_convert_dataset_to_dataframe",
    "collapsed": false
   },
   "source": "### Load Dataset as Snowpark DataFrame\n\nMaterialize the registered dataset from Feature Store into a Snowpark DataFrame for analysis and modeling.\n\n---\n\n#### **What This Does:**\n\n```python\nds_sp = ds.read.to_snowpark_dataframe()\n```\n\n**Converts the dataset object to a Snowpark DataFrame:**\n- **`ds`**: Feature Store Dataset object (metadata/reference)\n- **`ds.read.to_snowpark_dataframe()`**: Materializes the actual data\n- **`ds_sp`**: Snowpark DataFrame ready for manipulation\n\n**Key characteristics:**\n- ✅ Data stays in Snowflake (no movement to client)\n- ✅ Lazy evaluation (efficient query execution)\n- ✅ Can handle large datasets\n- ✅ Ready for transformations, splits, and ML pipelines\n\n---\n\n#### **Preview Data:**\n\n```python\nds_sp.show(5)\n```\n\nDisplays first 5 rows to verify the dataset contains:\n- ✅ Spine columns (LOAN_ID, TIMESTAMP, LOAN_PURPOSE_NAME)\n- ✅ All engineered features from Feature View\n- ✅ Target variable (MORTGAGERESPONSE)\n\n---\n\n#### **Dataset Object vs Snowpark DataFrame:**\n\n| `ds` (Dataset Object) | `ds_sp` (Snowpark DataFrame) |\n|-----------------------|------------------------------|\n| Feature Store metadata | Materialized data |\n| Registered definition | Ready for manipulation |\n| Reference/pointer | Actual rows and columns |\n| Can't transform directly | Can filter, join, transform |\n\n---\n\n**Next:** Use `ds_sp` for EDA, train/test split, preprocessing, and model training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdaa537-3fb9-476c-9153-3236edfdfcb3",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "convert_dataset_to_snowpark_and_pandas",
    "resultHeight": 239
   },
   "outputs": [],
   "source": [
    "ds_sp = ds.read.to_snowpark_dataframe()\n",
    "ds_sp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b3175a-709e-4f46-9ad0-d198338083c9",
   "metadata": {
    "name": "md_one_hot_encoding",
    "collapsed": false
   },
   "source": "### Preprocess Categorical Features\n\nApply one-hot encoding to convert categorical (string) columns into numerical format required by ML algorithms.\n\n---\n\n#### **What This Does:**\n\n**1. Identify Categorical Columns**\n```python\nOHE_COLS = ds_sp.select([col.name for col in ds_sp.schema if col.datatype == StringType()]).columns\n```\nAutomatically detects all string/categorical columns (e.g., `LOAN_PURPOSE_NAME`, `COUNTY_NAME`)\n\n**2. Apply One-Hot Encoding**\n```python\nsnowml_ohe = snowml.OneHotEncoder(input_cols=OHE_COLS, output_cols=OHE_COLS, drop_input_cols=True)\nds_sp_ohe = snowml_ohe.fit(ds_sp).transform(ds_sp)\n```\n- **`fit()`**: Learns all unique categories in each column\n- **`transform()`**: Creates binary (0/1) columns for each category\n- **`drop_input_cols=True`**: Removes original string columns\n\n**3. Clean Column Names**\n```python\nrename_dict = {col: col.replace('\"','').replace(' ', '_') for col in ds_sp_ohe.columns if '\"' in col}\nds_sp_ohe = ds_sp_ohe.rename(rename_dict)\n```\nRemoves quotes and replaces spaces with underscores for SQL compatibility\n\n---\n\n#### **Example Transformation:**\n\n**Before:** `LOAN_PURPOSE_NAME = \"Purchase\"`  \n**After:** `LOAN_PURPOSE_NAME_Purchase = 1`, `LOAN_PURPOSE_NAME_Refinance = 0`, `LOAN_PURPOSE_NAME_Home_Improvement = 0`\n\n---\n\n**Result:** `ds_sp_ohe` contains only numerical features, ready for train/test split and model training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e17036-7a69-4915-b025-49c900aeb46b",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "one_hot_encoding",
    "resultHeight": 360
   },
   "outputs": [],
   "source": [
    "import snowflake.ml.modeling.preprocessing as snowml\n",
    "from snowflake.snowpark.types import StringType\n",
    "\n",
    "OHE_COLS = ds_sp.select([col.name for col in ds_sp.schema if col.datatype ==StringType()]).columns\n",
    "OHE_POST_COLS = [i+\"_OHE\" for i in OHE_COLS]\n",
    "\n",
    "\n",
    "# Encode categoricals to numeric columns\n",
    "snowml_ohe = snowml.OneHotEncoder(input_cols=OHE_COLS, output_cols = OHE_COLS, drop_input_cols=True)\n",
    "ds_sp_ohe = snowml_ohe.fit(ds_sp).transform(ds_sp)\n",
    "\n",
    "#Rename columns to avoid double nested quotes and white space chars\n",
    "rename_dict = {}\n",
    "for i in ds_sp_ohe.columns:\n",
    "    if '\"' in i:\n",
    "        rename_dict[i] = i.replace('\"','').replace(' ', '_')\n",
    "\n",
    "ds_sp_ohe = ds_sp_ohe.rename(rename_dict)\n",
    "ds_sp_ohe.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9173cf9-014d-4241-94e1-ab0cbae61096",
   "metadata": {
    "name": "md_train_test_split",
    "collapsed": false
   },
   "source": "### Split Data into Training and Test Sets\n\nDivide the dataset into training (70%) and test (30%) sets for model development and evaluation.\n\n---\n\n#### **What This Does:**\n\n```python\ntrain, test = ds_sp_ohe.random_split(weights=[0.70, 0.30], seed=0)\n```\n\n**Parameters:**\n- **`weights=[0.70, 0.30]`**: 70% of data for training, 30% for testing\n- **`seed=0`**: Fixed random seed for reproducibility\n\n---\n\n#### **Why Split the Data?**\n\n- **Training set (70%)**: Used to train the model and learn patterns\n- **Test set (30%)**: Used to evaluate model performance on unseen data\n\n**Purpose:** Ensures the model can generalize to new data and prevents overfitting.\n\n---\n\n**Result:** Two separate Snowpark DataFrames ready for model training and evaluation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d834f6f3-ce15-405e-8fec-1d1bb5c224a6",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "train_test_split",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "train, test = ds_sp_ohe.random_split(weights=[0.70, 0.30], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff103e-5314-4e95-87ba-d784b1102f36",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "fill_na",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "train = train.fillna(0)\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "id": "1cd01ee7-ac14-45ed-af4e-dd60cc5f077a",
   "metadata": {
    "language": "python",
    "name": "validate_timestamps"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import col, count, when\n\n# Summary of TIMESTAMP column health\ntimestamp_check = train.select(\n    count(\"*\").alias(\"total_rows\"),\n    count(\"TIMESTAMP\").alias(\"non_null_timestamps\"),\n    count(when(col(\"TIMESTAMP\").is_null(), 1)).alias(\"null_timestamps\")\n).collect()[0]\n\nprint(f\"Total rows: {timestamp_check['TOTAL_ROWS']}\")\nprint(f\"Non-null timestamps: {timestamp_check['NON_NULL_TIMESTAMPS']}\")\nprint(f\"Null timestamps: {timestamp_check['NULL_TIMESTAMPS']}\")\nprint(f\"Timestamp coverage: {timestamp_check['NON_NULL_TIMESTAMPS'] / timestamp_check['TOTAL_ROWS'] * 100:.2f}%\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a0d0ecf3-8069-4696-9ac1-d2a7457b97b8",
   "metadata": {
    "name": "md_convert_to_pandas",
    "collapsed": false
   },
   "source": "### Convert to Pandas DataFrames\n\nConvert Snowpark DataFrames to pandas DataFrames for local processing.\n\n---\n\n```python\ntrain_pd = train.to_pandas()\ntest_pd = test.to_pandas()\n```\n\n**What this does:**\n- Downloads data from Snowflake to local memory as pandas DataFrames\n- Required for certain ML operations that need in-memory data structures\n- Enables use of pandas-based libraries and methods\n\n**Note:** Data is moved from Snowflake warehouse to the notebook's local environment. For large datasets, consider keeping data in Snowpark format when possible to leverage Snowflake's compute resources."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c917df7f-e277-4fbb-abf5-1a4433367e3b",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "convert_data_to_pandas"
   },
   "outputs": [],
   "source": [
    "train_pd = train.to_pandas()\n",
    "test_pd = test.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c05dc9-2efb-4c5f-995a-486ef926c6c5",
   "metadata": {
    "collapsed": false,
    "name": "model_training_md"
   },
   "source": "## Model Training\n\n### Train Baseline XGBoost Classifier\n\nDefine and train an **intentionally overfit** baseline XGBoost model to demonstrate the impact of poor hyperparameter tuning.\n\n---\n\n#### **Baseline Model Configuration:**\n\n```python\nxgb_base = XGBClassifier(\n    max_depth=50,\n    n_estimators=3,\n    learning_rate=0.75,\n    booster='gbtree'\n)\n```\n\n**Hyperparameters (Intentionally Suboptimal):**\n- **`max_depth=50`**: Extremely deep trees (typical: 3-10) → **overfitting**\n- **`n_estimators=3`**: Very few boosting rounds (typical: 50-500) → **underfitting individual trees**\n- **`learning_rate=0.75`**: Aggressive learning rate (typical: 0.01-0.3) → **overshooting optimal weights**\n- **`booster='gbtree'`**: Tree-based gradient boosting algorithm\n\n---\n\n#### **⚠️ Why These Parameters Are Poor:**\n\n| Parameter | Baseline Value | Typical Range | Problem |\n|-----------|----------------|---------------|---------|\n| `max_depth` | 50 | 3-10 | Trees memorize training data |\n| `n_estimators` | 3 | 50-500 | Too few trees to learn patterns |\n| `learning_rate` | 0.75 | 0.01-0.3 | Too aggressive, overshoots |\n\n**Expected Result:** High training accuracy, poor test accuracy (classic overfitting)\n\n---\n\n#### **🎯 Purpose of This Baseline:**\n\n1. **Establish comparison point** - Measure performance before optimization\n2. **Demonstrate overfitting** - Show train/test performance gap\n3. **Motivate HPO** - Create a clear need for hyperparameter optimization\n4. **Educational value** - Teach the importance of proper tuning\n\n\n**Next Steps:** Train this baseline model, evaluate on train/test sets, then compare against an optimized model trained with Distributed Hyperparameter Optimization (HPO)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4b5fba-b7a8-47ff-aaf6-076b9e78dcaf",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "define_model",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "#Define model config\n",
    "xgb_base = XGBClassifier(\n",
    "    max_depth=50,\n",
    "    n_estimators=3,\n",
    "    learning_rate = 0.75,\n",
    "    booster = 'gbtree')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63710ba4-df22-42ce-8179-6634c7f79184",
   "metadata": {
    "name": "md_train_base_model",
    "collapsed": false
   },
   "source": "### Prepare Training Data and Fit Baseline Model\n\nSplit the training data into features (X) and target (y), then train the baseline XGBoost classifier.\n\n---\n\n#### **Prepare Features and Target:**\n\n```python\nX_train_pd = train_pd.drop([\"TIMESTAMP\", \"LOAN_ID\", \"MORTGAGERESPONSE\"], axis=1)\ny_train_pd = train_pd.MORTGAGERESPONSE\n```\n\n**Features (`X_train_pd`)**: All engineered and one-hot encoded columns, excluding:\n- `TIMESTAMP` - Not used for prediction (temporal identifier only)\n- `LOAN_ID` - Entity identifier, not a predictive feature\n- `MORTGAGERESPONSE` - Target variable\n\n**Target (`y_train_pd`)**: `MORTGAGERESPONSE` - Binary label indicating loan approval (1) or denial (0)\n\n---\n\n#### **Train the Model:**\n\n```python\nxgb_base.fit(X_train_pd, y_train_pd)\n```\n\nTrains the XGBoost classifier on the training data using the suboptimal hyperparameters defined previously.\n\n---\n\n**Result:** A trained baseline model that will likely overfit the training data (to be evaluated in the next cell)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644f3295-2496-4fd0-ae95-922a78c5b944",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "train_base_model",
    "resultHeight": 1759
   },
   "outputs": [],
   "source": [
    "#Split train data into X, y\n",
    "X_train_pd = train_pd.drop([\"TIMESTAMP\", \"LOAN_ID\", \"MORTGAGERESPONSE\"],axis=1) #remove\n",
    "y_train_pd = train_pd.MORTGAGERESPONSE\n",
    "\n",
    "#train model\n",
    "xgb_base.fit(X_train_pd,y_train_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74433ebe-0c06-465a-b07f-8af60c2538b8",
   "metadata": {
    "name": "md_training_complete",
    "collapsed": false
   },
   "source": "### ✅ Model Training Complete\n\nThe output displays the trained XGBoost model object with its configuration parameters.\n\n#### **Confirmed Hyperparameters:**\n\n**Custom Settings (Intentionally Suboptimal):**\n- `max_depth=50` - Extremely deep trees (overfitting risk)\n- `n_estimators=3` - Very few boosting rounds\n- `learning_rate=0.75` - Aggressive learning rate\n- `booster='gbtree'` - Tree-based gradient boosting\n\n**Default Settings:**\n- Most other parameters set to `None` (using XGBoost defaults)\n- `enable_categorical=False` - All features treated as numerical\n- `random_state=None` - No fixed random seed\n\n---\n\n#### **What This Means:**\n\n✅ **Training successful** - Model is fitted and ready for predictions  \n✅ **Configuration active** - Your specified hyperparameters are applied  \n✅ **Using defaults** - Unspecified parameters use XGBoost's standard values\n\n---\n\n**Next:** Evaluate this baseline model's performance on training and test data to observe overfitting behavior."
  },
  {
   "cell_type": "markdown",
   "id": "d358856f-27e3-4296-94a8-84fe16ab2004",
   "metadata": {
    "name": "md_compute_predictions_and_perf_metrics",
    "collapsed": false
   },
   "source": "### Evaluate Baseline Model Performance on Training Data\n\nGenerate predictions on the training set and calculate classification metrics to assess baseline model performance.\n\n#### **Generate Predictions:**\n\n```python\ntrain_preds_base = xgb_base.predict(X_train_pd)\n```\n\nUses the trained baseline model to predict loan approval outcomes on the training data.\n\n\n#### **Calculate Performance Metrics:**\n\n```python\nf1_base_train = round(f1_score(y_train_pd, train_preds_base), 4)\nprecision_base_train = round(precision_score(y_train_pd, train_preds_base), 4)\nrecall_base_train = round(recall_score(y_train_pd, train_preds_base), 4)\n```\n\n**Metrics Explained:**\n- **F1 Score**: Harmonic mean of precision and recall (balanced measure)\n- **Precision**: Of all predicted approvals, how many were actually approved? (PPV)\n- **Recall**: Of all actual approvals, how many did we predict? (Sensitivity)\n\n\n#### **Expected Result:**\n\n**High training performance** (likely F1 > 0.95) - The overfit model should perform very well on data it was trained on.\n\n**Next step:** Evaluate on test data to reveal the performance drop-off and confirm overfitting."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ac861-fcf9-47b2-9c11-ec44ee2367e4",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "compute_predictions_and_perf_metrics"
   },
   "outputs": [],
   "source": "from sklearn.metrics import f1_score, precision_score, recall_score\ntrain_preds_base = xgb_base.predict(X_train_pd) # Generate predictions on training set\n\nf1_base_train = round(f1_score(y_train_pd, train_preds_base),4)\nprecision_base_train = round(precision_score(y_train_pd, train_preds_base),4)\nrecall_base_train = round(recall_score(y_train_pd, train_preds_base),4)\n\nprint(f'F1: {f1_base_train} \\nPrecision {precision_base_train} \\nRecall: {recall_base_train}')"
  },
  {
   "cell_type": "markdown",
   "id": "93777778-d2ba-42d5-88c4-a90ba18c5006",
   "metadata": {
    "collapsed": false,
    "name": "model_regisry_md",
    "resultHeight": 74
   },
   "source": "## Model Registry\n\nSnowflake's Model Registry provides centralized model management with metadata tracking, lifecycle management, versioning, and deployment capabilities.\n\n#### **Key Capabilities:**\n\n- 📝 **Log models** with metadata, metrics, and tags\n- 🔄 **Manage lifecycles** - version, promote, retire models\n- 🚀 **Serve models** from Snowflake Warehouses or Snowpark Container Services\n- 📊 **Enable monitoring** - track model performance and drift over time\n- 🔗 **Maintain lineage** - track data sources and dependencies\n\n#### **Initialize Model Registry:**\n\n```python\nfrom snowflake.ml.registry import Registry\n\nmodel_name = f\"MORTGAGE_LENDING_MLOPS_{VERSION_NUM}\"\n\nmodel_registry = Registry(\n    session=session, \n    database_name=DB, \n    schema_name=SCHEMA,\n    options={\"enable_monitoring\": True}\n)\n```\n\n**Configuration:**\n- **`model_name`**: Unique identifier for this model family (versioned with `VERSION_NUM`)\n- **`database_name`**: Database where models will be stored (`SANDBOX`)\n- **`schema_name`**: Schema for model objects (`MEDPACE_HOL`)\n- **`enable_monitoring=True`**: Enables drift detection and performance monitoring\n\n#### **What This Creates:**\n\nA Model Registry instance connected to your Snowflake account, ready to:\n- ✅ Log baseline and optimized model versions\n- ✅ Track metrics (F1, precision, recall)\n- ✅ Set tags (PROD, DEV, etc.)\n- ✅ Manage default versions\n- ✅ Monitor model performance over time\n\n**Next:** Log the baseline model with training metrics and metadata."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21678e59-deaf-4c2b-b01e-1c59fe31b10a",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "define_model_registry",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "#Create a snowflake model registry object \n",
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "# Define model name\n",
    "model_name = f\"MORTGAGE_LENDING_MLOPS_{VERSION_NUM}\"\n",
    "\n",
    "# Create a registry to log the model to\n",
    "model_registry = Registry(session=session, \n",
    "                          database_name=DB, \n",
    "                          schema_name=SCHEMA,\n",
    "                          options={\"enable_monitoring\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9637bc-5215-4c34-9583-f19a74956b1e",
   "metadata": {
    "name": "md_register_model_version",
    "collapsed": false
   },
   "source": "### Log Baseline Model to Model Registry\n\nRegister the baseline XGBoost model in Snowflake Model Registry with metadata, training metrics, and deployment configuration.\n\n#### **Idempotent Logging Pattern:**\n\nUses try/except to check if model version already exists before logging (safe to re-run).\n\n#### **Key Registration Parameters:**\n\n**`model=xgb_base`**: The trained baseline XGBoost classifier\n\n**`version_name='XGB_BASE'`**: Unique identifier for this model version\n\n**`sample_input_data`**: Sample features (100 rows) for:\n- Schema inference\n- Input validation\n- Lineage tracking (uses Snowpark DataFrame to maintain data lineage)\n\n**`comment`**: Documentation including hyperparameters and model description\n\n**`target_platforms=['WAREHOUSE', 'SNOWPARK_CONTAINER_SERVICES']`**:\n- Deploy to Snowflake Warehouses for batch inference\n- Deploy to SPCS for real-time serving\n\n**`enable_explainability=True`**: Enables SHAP value computation for model interpretability\n\n#### **Attach Training Metrics:**\n\nLogs training performance metrics (F1, Precision, Recall) to the model version for tracking and comparison against optimized models.\n\n**Result:** Baseline model is versioned, documented, and ready for deployment with full metadata and lineage tracking."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be41c3ac-49f0-4fd9-a557-9d8eb633f602",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "register_model_version",
    "resultHeight": 229
   },
   "outputs": [],
   "source": [
    "#Log the base model to the model registry (if not already there)\n",
    "base_version_name = 'XGB_BASE'\n",
    "\n",
    "try:\n",
    "    #Check for existing model\n",
    "    mv_base = model_registry.get_model(model_name).version(base_version_name)\n",
    "    print(\"Found existing model version!\")\n",
    "except:\n",
    "    print(\"Logging new model version...\")\n",
    "    #Log model to registry\n",
    "    mv_base = model_registry.log_model(\n",
    "        model_name=model_name,\n",
    "        model=xgb_base, \n",
    "        version_name=base_version_name,\n",
    "        sample_input_data = train.drop([\"TIMESTAMP\", \"LOAN_ID\", \"MORTGAGERESPONSE\"]).limit(100), #using snowpark df to maintain lineage\n",
    "        comment = f\"\"\"ML model for predicting loan approval likelihood.\n",
    "                    This model was trained using XGBoost classifier.\n",
    "                    Hyperparameters used were:\n",
    "                    max_depth={xgb_base.max_depth}, \n",
    "                    n_estimators={xgb_base.n_estimators}, \n",
    "                    learning_rate = {xgb_base.learning_rate}, \n",
    "                    algorithm = {xgb_base.booster}\n",
    "                    \"\"\",\n",
    "        target_platforms= [\"WAREHOUSE\", \"SNOWPARK_CONTAINER_SERVICES\"],\n",
    "        options= {\"enable_explainability\": True}\n",
    "\n",
    "    )\n",
    "    \n",
    "    #set metrics\n",
    "    mv_base.set_metric(metric_name=\"Train_F1_Score\", value=f1_base_train)\n",
    "    mv_base.set_metric(metric_name=\"Train_Precision_Score\", value=precision_base_train)\n",
    "    mv_base.set_metric(metric_name=\"Train_Recall_score\", value=recall_base_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c618222e-3008-407e-8cbc-951db6c95715",
   "metadata": {
    "name": "md_create_prod_tag",
    "collapsed": false
   },
   "source": "### Create PROD Tag\n\nCreate a Snowflake tag to mark production-ready model versions.\n\n---\n\n```python\nsession.sql(\"CREATE OR REPLACE TAG PROD\")\n```\n\n**What this does:**\n- Creates a reusable tag named `PROD` for identifying production models\n- `CREATE OR REPLACE` ensures idempotency (safe to re-run)\n\n**Purpose:** Tags enable model lifecycle management by marking which versions are deployed to production, allowing easy identification and governance of active models.\n\n---\n\n**Next:** Apply the PROD tag to the baseline model version."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2ddab-b02a-4e05-8121-4e97e49e0eea",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "create_prod_tag",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create tag for PROD model\n",
    "session.sql(\"CREATE OR REPLACE TAG PROD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7418be69-acfa-476e-bb25-4ebc055c0240",
   "metadata": {
    "name": "md_apply_prod_tag",
    "collapsed": false
   },
   "source": "### Apply PROD Tag and Model Documentation\n\nAdd model-level documentation and tag the baseline version as the production model.\n\n```python\nm = model_registry.get_model(model_name)\nm.comment = \"Loan approval prediction models\"\nm.set_tag(\"PROD\", base_version_name)\nm.show_tags()\n```\n\n**What this does:**\n\n**1. Get model object**: Retrieves the model family from registry\n\n**2. Add model-level comment**: Documents the overall purpose (applies to all versions)\n\n**3. Apply PROD tag**: Marks `XGB_BASE` version as the current production model\n- Key: `\"PROD\"`\n- Value: `base_version_name` (XGB_BASE)\n\n**4. Display tags**: Shows all tags applied to the model\n\n**Purpose:** Tags enable lifecycle management and governance by clearly identifying which model version is currently deployed in production. Multiple versions can exist, but only one is tagged as PROD.\n\n**Result:** Baseline model is now marked as the active production version and can be easily identified for deployment and monitoring."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0054df-0cd9-4e81-98b8-6564be86b4b9",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "create_PROD_tag",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Apply prod tag \n",
    "m = model_registry.get_model(model_name)\n",
    "m.comment = \"Loan approval prediction models\" #set model level comment\n",
    "m.set_tag(\"PROD\", base_version_name)\n",
    "m.show_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e294e-929d-4399-b2bb-d5d2d1dd043e",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "show_models",
    "resultHeight": 111
   },
   "outputs": [],
   "source": [
    "model_registry.show_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dfb281-9751-48a1-a76e-43ffffd9d099",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "show_model_versions",
    "resultHeight": 146
   },
   "outputs": [],
   "source": [
    "model_registry.get_model(model_name).show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1af8a1-7a92-455e-b9a1-8f2c699dfdeb",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "print_model_version_and_metrics",
    "resultHeight": 239
   },
   "outputs": [],
   "source": [
    "print(mv_base)\n",
    "print(mv_base.show_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecdf05c-b3b5-4755-bdff-fd187ef07f58",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "show_model_functions",
    "resultHeight": 2133
   },
   "outputs": [],
   "source": [
    "mv_base.show_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea52461-6a39-449e-a401-c78326f66f99",
   "metadata": {
    "name": "md_explain_outputs",
    "collapsed": false
   },
   "source": "### Model Functions Output Explained\n\nThe output shows **three functions** automatically created when the model was registered with `enable_explainability=True`.\n\n\n#### **Available Functions:**\n\n**1. EXPLAIN (Table Function)**\n- **Purpose**: Generate SHAP values for model interpretability\n- **Inputs**: 12 features (loan purpose, temporal, financial, geographic)\n- **Outputs**: 12 explanation columns (one per feature with `_explanation` suffix)\n- **SHAP values**: Positive = pushes toward approval, Negative = pushes toward denial\n- **Usage**: `mv_base.run(data, function_name=\"explain\")`\n\n**2. PREDICT (Function)**\n- **Purpose**: Generate binary class predictions\n- **Inputs**: Same 12 features\n- **Output**: Single column `output_feature_0` with binary prediction (0 = denied, 1 = approved)\n- **Usage**: `mv_base.run(data, function_name=\"predict\")`\n\n**3. PREDICT_PROBA (Function)**\n- **Purpose**: Generate probability scores for each class\n- **Inputs**: Same 12 features\n- **Outputs**: Two columns with probabilities [P(denial), P(approval)]\n- **Usage**: `mv_base.run(data, function_name=\"predict_proba\")`\n\n\n#### **Key Observations:**\n\n✅ **Consistent input schema** - All functions expect the same 12 features with specific data types  \n✅ **Multiple inference modes** - Choose based on use case (decision, confidence, or explanation)  \n✅ **Automatic generation** - Created by Model Registry during `log_model()`  \n✅ **Type safety** - Inputs and outputs have defined data types (INT8, DOUBLE, etc.)\n\n#### **When to Use Each:**\n\n| Function | Use Case | Example |\n|----------|----------|----------|\n| **PREDICT** | Final decisions | \"Should we approve this loan\" -> Yes/No |\n| **PREDICT_PROBA** | Risk assessment, threshold tuning | \"What's the confidence? Can we adjust thresholds?\" |\n| **EXPLAIN** | Debugging, compliance, interpretability | \"Why was this loan denied? Which factors mattered most?\" |"
  },
  {
   "cell_type": "markdown",
   "id": "285d8841-1435-478a-9a02-5b1f201cbc2d",
   "metadata": {
    "name": "md_predict_from_registry",
    "collapsed": false
   },
   "source": "### Run Batch Inference on Test Data\n\nUse the registered baseline model to generate predictions on the test dataset directly from Model Registry.\n\n---\n\n```python\nreg_preds = mv_base.run(test, function_name=\"predict\").rename(col('\"output_feature_0\"'), \"MORTGAGE_PREDICTION\")\nreg_preds.show(10)\n```\n\n**What this does:**\n\n**`mv_base.run(test, function_name=\"predict\")`**\n- Invokes the PREDICT function from Model Registry\n- Runs inference on the Snowpark `test` DataFrame\n- Executes in Snowflake warehouse (no data movement)\n- Returns predictions in column `output_feature_0`\n\n**`.rename(col('\"output_feature_0\"'), \"MORTGAGE_PREDICTION\")`**\n- Renames the output column to a more descriptive name\n- Note the escaped quotes: `'\"output_feature_0\"'` handles Snowflake identifier casing\n\n**`.show(10)`**\n- Displays first 10 predictions with all columns (features + prediction)\n\n---\n\n#### **Key Advantages of Model Registry Inference:**\n\n✅ **Warehouse execution** - Predictions run in Snowflake, no data movement  \n✅ **Version control** - Tied to specific model version (`XGB_BASE`)  \n✅ **Scalable** - Handles large datasets efficiently  \n✅ **Consistent** - Same preprocessing/inference pipeline as training  \n✅ **Traceable** - Full lineage from features to predictions\n\n---\n\n**Output:** Test dataset with added `MORTGAGE_PREDICTION` column (0 = denied, 1 = approved)\n\n**Next:** Evaluate test performance metrics to reveal overfitting."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf495261-a8a7-46be-b9c8-3f099268d154",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "predict_from_registry",
    "resultHeight": 351
   },
   "outputs": [],
   "source": [
    "reg_preds = mv_base.run(test, function_name = \"predict\").rename(col('\"output_feature_0\"'), \"MORTGAGE_PREDICTION\")\n",
    "reg_preds.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef61447-10e7-4a38-a429-3da3facf9ce7",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "compute_test_metrics"
   },
   "outputs": [],
   "source": [
    "#ds_sp_ohe = ds_sp_ohe.rename(col('\"LOAN_PURPOSE_NAME_Home improvement\"'), \"LOAN_PURPOSE_NAME_Home_improvement\")\n",
    "\n",
    "preds_pd = reg_preds.select([\"MORTGAGERESPONSE\", \"MORTGAGE_PREDICTION\"]).to_pandas()\n",
    "f1_base_test = round(f1_score(preds_pd.MORTGAGERESPONSE, preds_pd.MORTGAGE_PREDICTION),4)\n",
    "precision_base_test = round(precision_score(preds_pd.MORTGAGERESPONSE, preds_pd.MORTGAGE_PREDICTION),4)\n",
    "recall_base_test = round(recall_score(preds_pd.MORTGAGERESPONSE, preds_pd.MORTGAGE_PREDICTION),4)\n",
    "\n",
    "#log metrics to model registry model\n",
    "mv_base.set_metric(metric_name=\"Test_F1_Score\", value=f1_base_test)\n",
    "mv_base.set_metric(metric_name=\"Test_Precision_Score\", value=precision_base_test)\n",
    "mv_base.set_metric(metric_name=\"Test_Recall_score\", value=recall_base_test)\n",
    "\n",
    "print(f'F1: {f1_base_test} \\nPrecision {precision_base_test} \\nRecall: {recall_base_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b477885-35ce-486d-9e86-7d0cc9d48454",
   "metadata": {
    "collapsed": false,
    "name": "HPO_MD"
   },
   "source": [
    "# Oh no! Our model's performance seems to have dropped off significantly from training to our test set. \n",
    "## This is evidence that our model is overfit - can we fix this with Distributed Hyperparameter Optimization??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2a36cd-c0c1-41c7-857f-460e8c8a30e2",
   "metadata": {
    "name": "md_prepare_for_HPO",
    "collapsed": false
   },
   "source": "### Prepare Data for Distributed Hyperparameter Optimization\n\nSplit train and test Snowpark DataFrames into features (X) and target (y) for use in Snowflake's distributed HPO framework.\n\n**What this does:**\n\n**Features (X):** All ML features, excluding:\n- `MORTGAGERESPONSE` - Target variable\n- `TIMESTAMP` - Not predictive\n- `LOAN_ID` - Entity identifier\n\n**Target (y):** Binary loan approval outcome (0 = denied, 1 = approved)\n\n#### **Key Difference from Baseline Model:**\n\n- **Baseline **: Used pandas DataFrames for local training\n- **HPO **: Uses Snowpark DataFrames for distributed training in Snowflake warehouse\n\n\n**Result:** Clean X/y splits ready for distributed hyperparameter optimization across multiple compute nodes.\n\n**Next:** Configure and run distributed HPO to find optimal hyperparameters."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47e068d-7e1d-4c74-8289-d03ea8ab3c7e",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "setup_x_and_y",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(\"MORTGAGERESPONSE\", \"TIMESTAMP\", \"LOAN_ID\")\n",
    "y_train = train.select(\"MORTGAGERESPONSE\")\n",
    "X_test = test.drop(\"MORTGAGERESPONSE\",\"TIMESTAMP\", \"LOAN_ID\")\n",
    "y_test = test.select(\"MORTGAGERESPONSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b57d0b-1cb9-4359-bf19-0f0ca10eefea",
   "metadata": {
    "name": "md_define_HPO_Config",
    "collapsed": false
   },
   "source": "### Configure Distributed Hyperparameter Optimization (HPO)\n\nSet up Snowflake's distributed HPO framework to find optimal hyperparameters and fix the overfitting problem observed in the baseline model.\n\n#### **1. Prepare Dataset Connectors:**\n\n```python\ndataset_map = {\n    \"x_train\": DataConnector.from_dataframe(X_train),\n    \"y_train\": DataConnector.from_dataframe(y_train),\n    \"x_test\": DataConnector.from_dataframe(X_test),\n    \"y_test\": DataConnector.from_dataframe(y_test)\n}\n```\n\nWraps Snowpark DataFrames in `DataConnector` objects for distributed HPO consumption.\n\n#### **2. Define Training Function:**\n\n```python\ndef train_func():\n    tuner_context = get_tuner_context()\n    config = tuner_context.get_hyper_params()  # Get trial's hyperparameters\n    dm = tuner_context.get_dataset_map()       # Get datasets\n    \n    model = XGBClassifier(**config, random_state=42)\n    model.fit(...)\n    f1_metric = f1_score(...)\n    tuner_context.report(metrics={\"f1_score\": f1_metric}, model=model)\n```\n\n**Executed for each HPO trial** with different hyperparameter combinations.\n\n#### **3. Configure Hyperparameter Search Space:**\n\n```python\nsearch_space = {\n    \"max_depth\": tune.randint(1, 10),           # Much more conservative than baseline (50)\n    \"learning_rate\": tune.uniform(0.01, 0.1),   # Lower range than baseline (0.75)\n    \"n_estimators\": tune.randint(50, 100)       # More trees than baseline (3)\n}\n```\n\n**Comparison to baseline:**\n\n| Parameter | Baseline | Search Space | Why Better |\n|-----------|----------|--------------|------------|\n| `max_depth` | 50 | 1-10 | Prevents overfitting |\n| `learning_rate` | 0.75 | 0.01-0.1 | More gradual learning |\n| `n_estimators` | 3 | 50-100 | More trees = better patterns |\n\n#### **4. Configure Tuner:**\n\n```python\ntuner = tune.Tuner(\n    train_func=train_func,\n    search_space=search_space,\n    tuner_config=tune.TunerConfig(\n        metric=\"f1_score\",\n        mode=\"max\",                                      # Maximize F1\n        search_alg=RandomSearch(random_state=101),       # Search strategy\n        num_trials=8,                                    # Test 8 combinations\n        max_concurrent_trials=psutil.cpu_count(logical=False)  # Parallel execution\n    )\n)\n```\n\n**Key settings:**\n- **Optimize for**: F1 score (balance precision and recall)\n- **Strategy**: Random search across parameter space\n- **Trials**: 8 different hyperparameter combinations\n- **Parallel**: Runs across all available CPU cores (distributed execution)\n\n#### **🚀 What Happens Next:**\n\nWhen you run `tuner.run(dataset_map)`:\n1. Launches 8 parallel training jobs in Snowflake warehouse\n2. Each job tests different hyperparameter combinations\n3. Evaluates F1 score for each combination\n4. Returns the best performing model\n\n**Goal:** Find hyperparameters that generalize well to test data, unlike the overfit baseline model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff76e05-38f5-47cf-ab5d-9eefed09bd71",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "define_HPO_config"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.data import DataConnector\n",
    "from snowflake.ml.modeling.tune import get_tuner_context\n",
    "from snowflake.ml.modeling import tune\n",
    "from entities import search_algorithm\n",
    "import psutil\n",
    "\n",
    "#Define dataset map\n",
    "dataset_map = {\n",
    "    \"x_train\": DataConnector.from_dataframe(X_train),\n",
    "    \"y_train\": DataConnector.from_dataframe(y_train),\n",
    "    \"x_test\": DataConnector.from_dataframe(X_test),\n",
    "    \"y_test\": DataConnector.from_dataframe(y_test)\n",
    "    }\n",
    "\n",
    "\n",
    "# Define a training function, with any models you choose within it.\n",
    "def train_func():\n",
    "    # A context object provided by HPO API to expose data for the current HPO trial\n",
    "    tuner_context = get_tuner_context()\n",
    "    config = tuner_context.get_hyper_params()\n",
    "    dm = tuner_context.get_dataset_map()\n",
    "\n",
    "    model = XGBClassifier(**config, random_state=42)\n",
    "    model.fit(dm[\"x_train\"].to_pandas().sort_index(), dm[\"y_train\"].to_pandas().sort_index())\n",
    "    f1_metric = f1_score(\n",
    "        dm[\"y_train\"].to_pandas().sort_index(), model.predict(dm[\"x_train\"].to_pandas().sort_index())\n",
    "    )\n",
    "    tuner_context.report(metrics={\"f1_score\": f1_metric}, model=model)\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    train_func=train_func,\n",
    "    search_space={\n",
    "        \"max_depth\": tune.randint(1, 10),\n",
    "        \"learning_rate\": tune.uniform(0.01, 0.1),\n",
    "        \"n_estimators\": tune.randint(50, 100),\n",
    "    },\n",
    "    tuner_config=tune.TunerConfig(\n",
    "        metric=\"f1_score\",\n",
    "        mode=\"max\",\n",
    "        search_alg=search_algorithm.RandomSearch(random_state=101),\n",
    "        num_trials=8, #run 8 trial runs\n",
    "        max_concurrent_trials=psutil.cpu_count(logical=False) # Use all available CPUs to run distributed HPO across. GPUs can also be used here! \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b88e99-9d2a-44cd-81f9-d53fd1321daf",
   "metadata": {
    "language": "python",
    "name": "run_hpo",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "#Train several model candidates (note this may take 1-2 minutes)\n",
    "tuner_results = tuner.run(dataset_map=dataset_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fced8c-400d-4d4b-b416-c78ee29a3f74",
   "metadata": {
    "name": "md_inspect_hpo_output",
    "collapsed": false
   },
   "source": "### Select Best Model from HPO\n\nRetrieve the best performing model from the distributed hyperparameter optimization results.\n\n```python\ntuned_model = tuner_results.best_model\n```\n\nExtracts the XGBoost model with the hyperparameters that achieved the highest F1 score across all 8 trials.\n\n#### **✅ Optimized Hyperparameters Found:**\n\n| Parameter | Baseline (Overfit) | Optimized (HPO) | Improvement |\n|-----------|-------------------|-----------------|-------------|\n| `max_depth` | 50 | **7** | ✅ Much shallower (prevents overfitting) |\n| `learning_rate` | 0.75 | **0.043** | ✅ More gradual learning |\n| `n_estimators` | 3 | **93** | ✅ More trees for better patterns |\n| `random_state` | None | **42** | ✅ Reproducible results |\n\n#### **Key Observations:**\n\n**`max_depth=7`** (vs baseline 50)\n- Prevents trees from memorizing training data\n- Better generalization to unseen data\n\n**`learning_rate=0.043`** (vs baseline 0.75)\n- Smaller, more conservative updates\n- Reduces risk of overshooting optimal weights\n\n**`n_estimators=93`** (vs baseline 3)\n- Many more trees to learn complex patterns\n- Ensemble approach improves robustness\n\n#### **Result:**\n\nA well-tuned XGBoost classifier that should demonstrate:\n- ✅ **Modest training accuracy** (not perfect - doesn't memorize)\n- ✅ **Strong test accuracy** (good generalization)\n- ✅ **Minimal overfitting** (small train/test performance gap)\n\n**Next:** Evaluate this optimized model on train and test sets to confirm improved generalization compared to the baseline."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee37c42-3de7-476a-b7c0-d56952dac385",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "inspect_hpo_params"
   },
   "outputs": [],
   "source": [
    "#Select best model results and inspect configuration\n",
    "tuned_model = tuner_results.best_model\n",
    "tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4a6c2-674e-4d02-afdb-8ebf10cffdc4",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "compute_hpo_train_predictions_and_metrics"
   },
   "outputs": [],
   "source": [
    "#Generate predictions\n",
    "xgb_opt_preds = tuned_model.predict(train_pd.drop([\"TIMESTAMP\", \"LOAN_ID\", \"MORTGAGERESPONSE\"],axis=1))\n",
    "\n",
    "#Generate performance metrics\n",
    "f1_opt_train = round(f1_score(train_pd.MORTGAGERESPONSE, xgb_opt_preds),4)\n",
    "precision_opt_train = round(precision_score(train_pd.MORTGAGERESPONSE, xgb_opt_preds),4)\n",
    "recall_opt_train = round(recall_score(train_pd.MORTGAGERESPONSE, xgb_opt_preds),4)\n",
    "\n",
    "print(f'Train Results: \\nF1: {f1_opt_train} \\nPrecision {precision_opt_train} \\nRecall: {recall_opt_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee80c48-d521-4b77-8841-54ba35ecd4b6",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "compute_hpo_test_predictions_and_metrics"
   },
   "outputs": [],
   "source": [
    "#Generate test predictions\n",
    "xgb_opt_preds_test = tuned_model.predict(test_pd.drop([\"TIMESTAMP\", \"LOAN_ID\", \"MORTGAGERESPONSE\"],axis=1))\n",
    "\n",
    "#Generate performance metrics on test data\n",
    "f1_opt_test = round(f1_score(test_pd.MORTGAGERESPONSE, xgb_opt_preds_test),4)\n",
    "precision_opt_test = round(precision_score(test_pd.MORTGAGERESPONSE, xgb_opt_preds_test),4)\n",
    "recall_opt_test = round(recall_score(test_pd.MORTGAGERESPONSE, xgb_opt_preds_test),4)\n",
    "\n",
    "print(f'Test Results: \\nF1: {f1_opt_test} \\nPrecision {precision_opt_test} \\nRecall: {recall_opt_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a1a670-52e3-4d77-ac3a-db830e22fdcf",
   "metadata": {
    "collapsed": false,
    "name": "HPO_performance_reaction"
   },
   "source": [
    "# Here we see the HPO model has a more modest train accuracy than our base model - but the peformance doesn't drop off during testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d501cf7d-4965-4b9f-8b16-edab897d0e18",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "log_hpo_model",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "#Log the optimized model to the model registry (if not already there)\n",
    "optimized_version_name = 'XGB_Optimized'\n",
    "\n",
    "try:\n",
    "    #Check for existing model\n",
    "    mv_opt = model_registry.get_model(model_name).version(optimized_version_name)\n",
    "    print(\"Found existing model version!\")\n",
    "except:\n",
    "    #Log model to registry\n",
    "    print(\"Logging new model version...\")\n",
    "    mv_opt = model_registry.log_model(\n",
    "        model_name=model_name,\n",
    "        model=tuned_model, \n",
    "        version_name=optimized_version_name,\n",
    "        sample_input_data = train.drop([\"TIMESTAMP\", \"LOAN_ID\", \"MORTGAGERESPONSE\"]).limit(100),\n",
    "        comment = f\"\"\"HPO ML model for predicting loan approval likelihood.\n",
    "            This model was trained using XGBoost classifier.\n",
    "            Optimized hyperparameters used were:\n",
    "            max_depth={tuned_model.max_depth}, \n",
    "            n_estimators={tuned_model.n_estimators}, \n",
    "            learning_rate = {tuned_model.learning_rate}, \n",
    "            \"\"\",\n",
    "        target_platforms= [\"WAREHOUSE\", \"SNOWPARK_CONTAINER_SERVICES\"],\n",
    "        options= {\"enable_explainability\": True}\n",
    "\n",
    "        \n",
    "\n",
    "    )\n",
    "    #Set metrics\n",
    "    mv_opt.set_metric(metric_name=\"Train_F1_Score\", value=f1_opt_train)\n",
    "    mv_opt.set_metric(metric_name=\"Train_Precision_Score\", value=precision_opt_train)\n",
    "    mv_opt.set_metric(metric_name=\"Train_Recall_score\", value=recall_opt_train)\n",
    "\n",
    "    mv_opt.set_metric(metric_name=\"Test_F1_Score\", value=f1_opt_test)\n",
    "    mv_opt.set_metric(metric_name=\"Test_Precision_Score\", value=precision_opt_test)\n",
    "    mv_opt.set_metric(metric_name=\"Test_Recall_score\", value=recall_opt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c028b9-b590-45b4-9884-35ee206bca0d",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "inspect_current_default_version"
   },
   "outputs": [],
   "source": [
    "#Here we see the BASE version is our default version\n",
    "model_registry.get_model(model_name).default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac97a9-7af4-4331-bb0d-cf6ecc4a77f6",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "promote_optimized_version_to_default"
   },
   "outputs": [],
   "source": [
    "#Now we'll set the optimized model to be the default model version going forward\n",
    "model_registry.get_model(model_name).default = optimized_version_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04efcee-27e6-4423-b669-849bec7cc8fb",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "see_updated_model_versions"
   },
   "outputs": [],
   "source": [
    "#Now we see our optimized version we have now recently promoted to our DEFAULT model version\n",
    "model_registry.get_model(model_name).default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc92f7f-5f02-4cc5-82d0-758f65f2d485",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "update_model_tags"
   },
   "outputs": [],
   "source": [
    "#we'll now update the PROD tagged model to be the optimized model version rather than our overfit base version\n",
    "m.unset_tag(\"PROD\")\n",
    "m.set_tag(\"PROD\", optimized_version_name)\n",
    "m.show_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fff15e-5f49-4d4f-a02a-93e8f3114b11",
   "metadata": {
    "collapsed": false,
    "name": "explainability_MD"
   },
   "source": "## Model Explainability with SHAP Values\n\nNow that both baseline and optimized models are deployed, use Snowflake's built-in explainability features to understand how input features impact model predictions.\n\n### **What is Model Explainability?**\n\n**SHAP (SHapley Additive exPlanations)** values quantify each feature's contribution to individual predictions:\n- **Positive values**: Feature pushes prediction toward approval\n- **Negative values**: Feature pushes prediction toward denial\n- **Magnitude**: Strength of the feature's impact\n\n\n### **Generate SHAP Values:**\n\n```python\n# Create sample of 2,500 test records\ntest_pd_sample = test_pd.rename(columns=rename_dict).sample(n=2500, random_state=100).reset_index(drop=True)\n\n# Compute SHAP values for baseline model\nbase_shap_pd = mv_base.run(test_pd_sample, function_name=\"explain\")\n\n# Compute SHAP values for optimized model\nopt_shap_pd = mv_opt.run(test_pd_sample, function_name=\"explain\")\n```\n\n**What this does:**\n\n**1. Sample test data**: 2,500 records for efficient SHAP computation (SHAP is computationally expensive)\n\n**2. Rename columns**: Apply `rename_dict` to match model's expected input schema\n\n**3. Generate SHAP values**: Call `explain` function for both models\n   - `base_shap_pd`: Explanations from overfit baseline model\n   - `opt_shap_pd`: Explanations from optimized HPO model\n\n### **Output Structure:**\n\nEach SHAP DataFrame contains **12 explanation columns** (one per feature):\n- LOAN_PURPOSE_NAME_HOME_IMPROVEMENT_explanation\n- LOAN_PURPOSE_NAME_HOME_PURCHASE_explanation\n- LOAN_PURPOSE_NAME_REFINANCING_explanation\n- MONTH_explanation\n- DAY_OF_YEAR_explanation\n- DOTW_explanation\n- LOAN_AMOUNT_explanation\n- INCOME_explanation\n- INCOME_LOAN_RATIO_explanation\n- MEAN_COUNTY_INCOME_explanation\n- HIGH_INCOME_FLAG_explanation\n- AVG_THIRTY_DAY_LOAN_als differences in how models make decisions\n\n### **Key Benefits:**\n\n✅ **Built-in to Model Registry** - No additional SHAP library configuration  \n✅ **Scalable** - Computes in Snowflake warehouse  \n✅ **Versioned** - Each model version has consistent explainability  \n✅ **Compliance-ready** - Explain individual predictions for regulatory requirements\n\n\n**Next:** Visualize SHAP values to understand feature importance and model behavior.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914f5cd6-d254-42d4-a0be-9848c9d09d4a",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "compute_shap_vals",
    "resultHeight": 0,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "#create a sample of 1000 records\n",
    "test_pd_sample=test_pd.rename(columns=rename_dict).sample(n=2500, random_state = 100).reset_index(drop=True)\n",
    "\n",
    "#Compute shapley values for each model\n",
    "base_shap_pd = mv_base.run(test_pd_sample, function_name=\"explain\")\n",
    "opt_shap_pd = mv_opt.run(test_pd_sample, function_name=\"explain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a95612-9620-438f-bb56-aa3842c117dc",
   "metadata": {
    "name": "md_built_in_viz",
    "collapsed": false
   },
   "source": "### Visualize SHAP Values with Built-in Snowflake Functions\n\nUse Snowflake's built-in visualization functions to interpret how features influence the baseline model's predictions.\n\n#### **Prepare Feature DataFrame:**\n\n```python\nfeat_df = test_pd_sample.drop([\"MORTGAGERESPONSE\", \"TIMESTAMP\", \"LOAN_ID\"], axis=1)\n```\n\nExtracts only the 12 ML features (excludes target and identifiers) for visualization.\n\n#### **Generate Influence Sensitivity Plot:**\n\n```python\nexplain_visualize.plot_influence_sensitivity(base_shap_pd, feat_df, figsize=(1500, 500))\n```\n\n**What this creates:**\n- **SHAP dependence scatter plot** showing the relationship between feature values and their SHAP contributions\n- **X-axis**: Feature values (e.g., INCOME amounts)\n- **Y-axis**: SHAP values (positive = pushes toward approval, negative = toward denial)\n- **Interactive**: In Snowflake Notebooks, includes dropdown to select which feature to visualize\n\n**Purpose:** Understand how changes in feature values influence predictions (e.g., \"Does higher income always increase approval probability?\")\n\n#### **Additional Built-in Visualizations (Optional):**\n\n**1. Force Plot** - Single prediction explanation:\n```python\nexplain_visualize.plot_force(base_shap_pd.iloc[0], feat_df.iloc[0], figsize=(1500, 500))\n```\n- Shows how each feature pushes one specific prediction higher or lower\n- Red arrows = positive contributions (toward approval)\n- Blue arrows = negative contributions (toward denial)\n- Visualizes the path from base prediction to final prediction\n\n**2. Violin Plot** - Feature importance distribution:\n```python\nexplain_visualize.plot_violin(base_shap_pd, feat_df, figsize=(1400, 100))\n```\n- Shows distribution and range of SHAP values for each feature\n- Sorted by absolute mean SHAP value (most influential features at top)\n- Reveals which features have the most significant impact overall\n\n#### **Key Benefits:**\n\n✅ **Built-in to Snowflake** - No external SHAP library configuration  \n✅ **Interactive** (in Snowflake Notebooks) - Dropdown feature selection  \n✅ **Multiple perspectives** - Dependence, force, and distribution views  \n✅ **Production-ready** - Scalable visualizations for model debugging\n\n#### **When to Use Each:**\n\n| Visualization | Use Case |\n|---------------|----------|\n| **Influence Sensitivity** | Understand feature-prediction relationships across dataset |\n| **Force Plot** | Explain a single prediction to a stakeholder |\n| **Violin Plot** | Identify most important features overall |\n\n**Result:** Visual insights into which features drive the baseline model's decisions and how they interact with predictions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f606c231-2e6a-44ec-a17c-88bb5a3b6494",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "builtin_visualizations",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.monitoring import explain_visualize\n",
    "\n",
    "feat_df=test_pd_sample.drop([\"MORTGAGERESPONSE\",\"TIMESTAMP\", \"LOAN_ID\"],axis=1)\n",
    "\n",
    "explain_visualize.plot_influence_sensitivity(base_shap_pd, feat_df, figsize=(1500, 500))\n",
    "\n",
    "#Optionally test out other built-in functionality \n",
    "# explain_visualize.plot_force(base_shap_pd.iloc[0], feat_df.iloc[0], figsize=(1500, 500))\n",
    "# explain_visualize.plot_violin(base_shap_pd, feat_df, figsize=(1400, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7cfb33-1062-487d-83bc-e3d32835e0d9",
   "metadata": {
    "collapsed": false,
    "name": "shap_viz"
   },
   "source": [
    "### In addition to built-in visualization capabilities you can always use open source packages like shap for additional visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e0dcc-a850-474a-b475-f05a77619731",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "base_shap_summary_plot",
    "resultHeight": 571
   },
   "outputs": [],
   "source": [
    "import shap \n",
    "\n",
    "shap.summary_plot(np.array(base_shap_pd.astype(float)), \n",
    "                  test_pd_sample.drop([\"LOAN_ID\",\"MORTGAGERESPONSE\", \"TIMESTAMP\"], axis=1), \n",
    "                  feature_names = test_pd_sample.drop([\"LOAN_ID\",\"MORTGAGERESPONSE\", \"TIMESTAMP\"], axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67469a84-3d44-49e4-8d6e-5cd8a6e8a633",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "opt_shap_summary_plot",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(np.array(opt_shap_pd.astype(float)), \n",
    "                  test_pd_sample.drop([\"LOAN_ID\",\"MORTGAGERESPONSE\", \"TIMESTAMP\"], axis=1), \n",
    "                  feature_names = test_pd_sample.drop([\"LOAN_ID\",\"MORTGAGERESPONSE\", \"TIMESTAMP\"], axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a0d4c3-750c-4ae0-9812-85b677db6986",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "create_all_shap_dfs"
   },
   "outputs": [],
   "source": [
    "#Merge shap vals and actual vals together for easier plotting below\n",
    "all_shap_base = test_pd_sample.merge(base_shap_pd, right_index=True, left_index=True, how='outer')\n",
    "all_shap_opt = test_pd_sample.merge(opt_shap_pd, right_index=True, left_index=True, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4d8695-ef4c-4982-94be-7e4d9365bec1",
   "metadata": {
    "name": "md_income_expl",
    "collapsed": false
   },
   "source": "### Compare INCOME Feature Impact: Base vs Optimized Models\n\nCreate side-by-side scatter plots to visualize how the INCOME feature influences predictions differently in the baseline (overfit) and optimized models.\n\n#### **Data Preparation:**\n\n```python\nasb_filtered = all_shap_base[(all_shap_base.INCOME>0) & (all_shap_base.INCOME<250000)]\naso_filtered = all_shap_opt[(all_shap_opt.INCOME>0) & (all_shap_opt.INCOME<250000)]\n```\n\n**Filter outliers** to focus on the main income range ($0-$250K) for clearer visualization.\n\n- `all_shap_base`: SHAP explanations from baseline model\n- `all_shap_opt`: SHAP explanations from optimized model\n\n#### **Visualization:**\n\n```python\nsns.scatterplot()  # Plot SHAP values for each income level\nsns.regplot()      # Add regression line to show overall trend\n```\n\n**Left Plot (Base Model):**\n- Blue scatter points with red trend line\n- Shows INCOME vs INCOME_explanation relationship for overfit model\n\n**Right Plot (Optimized Model):**\n- Orange scatter points with blue trend line\n- Shows same relationship for HPO-tuned model\n\n#### **What to Look For:**\n\n✅ **Scatter tightness** - How consistent are SHAP values for similar incomes?  \n✅ **Trend line slope** - Does higher income lead to higher approval influence?  \n✅ **Y-axis range** - How extreme are the SHAP values?  \n✅ **Outliers** - Are there many unusual cases?\n\n\n**Goal:** Demonstrate that hyperparameter optimization produces more stable, interpretable feature impacts compared to the overfit baseline.\n\n\nsns.scatterplot()  # Plot SHAP values for each income level\nsns.regplot()      # Add regression line to show overall trendeter optimization in creating generalizable, production-ready predictions.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938441fd-9ae3-4f97-9a54-b7e4c74738ac",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "plot_income_explanation",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#filter data down to strip outliers\n",
    "asb_filtered = all_shap_base[(all_shap_base.INCOME>0) & (all_shap_base.INCOME<250000)]\n",
    "aso_filtered = all_shap_opt[(all_shap_opt.INCOME>0) & (all_shap_opt.INCOME<250000)]\n",
    "\n",
    "# Set up the figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "fig.suptitle(\"INCOME EXPLANATION\")\n",
    "# Plot side-by-side boxplots\n",
    "sns.scatterplot(data = asb_filtered, x ='INCOME', y = 'INCOME_explanation', ax=axes[0])\n",
    "sns.regplot(data = asb_filtered, x =\"INCOME\", y = 'INCOME_explanation', scatter=False, color='red', line_kws={\"lw\":2},ci =100, lowess=False, ax =axes[0])\n",
    "\n",
    "axes[0].set_title('Base Model')\n",
    "sns.scatterplot(data = aso_filtered, x ='INCOME', y = 'INCOME_explanation',color = \"orange\", ax = axes[1])\n",
    "sns.regplot(data = aso_filtered, x =\"INCOME\", y = 'INCOME_explanation', scatter=False, color='blue', line_kws={\"lw\":2},ci =100, lowess=False, ax =axes[1])\n",
    "axes[1].set_title('Opt Model')\n",
    "\n",
    "# Customize and show the plot\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Income\")\n",
    "    ax.set_ylabel(\"Influence\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3faedb-a82e-43ab-83aa-4f5411c728e2",
   "metadata": {
    "name": "md_plot_explanation",
    "collapsed": false
   },
   "source": "\n### Results: Dramatic Improvement in Model Stability\n\nComparing SHAP value distributions reveals the success of hyperparameter optimization in creating generalizable, production-ready predictions.\n\n#### **🔴 Base Model (Left Plot - Blue/Red)**\n\n**Observations:**\n\n**Extreme Variance**\n- SHAP values range from **-1.5 to +1.0** (2.5 unit spread)\n- Same income level shows wildly different impacts\n- Example: $100K income → SHAP ranges from -1.0 to +0.8\n\n**Weak Positive Trend**\n- Red line shows slight upward slope\n- But trend is **overwhelmed by scatter**\n- Poor predictive consistency\n\n**Unrealistic Patterns**\n- Many low-income cases have positive SHAP (pushes toward approval)\n- Many high-income cases have negative SHAP (pushes toward denial)\n- **Doesn't align with business logic**\n\n**🚨 Diagnosis:** Classic overfitting - model memorized training examples rather than learning generalizable patterns.\n\n#### **🟢 Optimized Model (Right Plot - Orange/Blue)**\n\n**Observations:**\n\n**Dramatically Reduced Variance**\n- SHAP values range from **-0.15 to +0.10** (0.25 unit spread)\n- **10x reduction in variance** compared to baseline\n- Consistent treatment of similar income levels\n\n**Nearly Flat Trend**\n- Blue line is almost horizontal\n- Income has **modest, stable influence**\n- Reflects sophisticated feature interactions\n\n**Few Outliers**\n- Only 2-3 points exceed ±0.10\n- Most values cluster tightly around zero\n\n**✅ Diagnosis:** Well-generalized model with stable, interpretable patterns.\n\n\n#### **📊 Key Comparisons**\n\n| Metric | Base Model | Optimized Model | Improvement |\n|--------|------------|-----------------|-------------|\n| **SHAP Range** | 2.5 units | 0.25 units | **10x reduction** |\n| **Consistency** | Chaotic | Stable | **Predictable** |\n| **Outliers** | Many extremes | Few exceptions | **Robust** |\n| **Business Logic** | Violated | Aligned | **Trustworthy** |\n\n\n#### **💡 Why is the Optimized Trend Nearly Flat?**\n\n**This is actually a GOOD sign!**\n\n1. **Feature Interactions**: Model learned that income's effect is **contextual**\n   - High income + large loan ≠ High income + small loan\n   - Income matters differently based on other factors\n\n2. **Engineered Features Handle the Heavy Lifting**:\n   - `INCOME_LOAN_RATIO` captures relative affordability\n   - `HIGH_INCOME_FLAG` captures threshold effects\n   - `MEAN_COUNTY_INCOME` provides geographic context\n   - Raw INCOME becomes less critical\n\n3. **Ensemble Learning** (93 trees): No single feature dominates, leading to balanced, robust predictions\n\n\n#### **🎯 Conclusion**\n\n**Hyperparameter optimization successfully fixed the overfitting problem:**\n\n- ✅ **Stable predictions** - Similar applicants get similar treatment\n- ✅ **Fair decisions** - Consistent feature impacts\n- ✅ **Production-ready** - Predictable, explainable behavior\n- ✅ **Better generalization** - Learns patterns, not memorizes examples\n\n**This visualization proves the optimized model is ready for deployment.**\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2298b1f8-0495-42e1-b668-0dcd03d8bb7c",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "plot_loan_amount_explanation"
   },
   "outputs": [],
   "source": [
    "#filter data down to strip outliers\n",
    "asb_filtered = all_shap_base[all_shap_base.LOAN_AMOUNT<2000000]\n",
    "aso_filtered = all_shap_opt[all_shap_opt.LOAN_AMOUNT<2000000]\n",
    "\n",
    "\n",
    "# Set up the figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "fig.suptitle(\"LOAN_AMOUNT EXPLANATION\")\n",
    "# Plot side-by-side boxplots\n",
    "sns.scatterplot(data = asb_filtered, x ='LOAN_AMOUNT', y = 'LOAN_AMOUNT_explanation', ax=axes[0])\n",
    "sns.regplot(data = asb_filtered, x =\"LOAN_AMOUNT\", y = 'LOAN_AMOUNT_explanation', scatter=False, color='red', line_kws={\"lw\":2},ci =100, lowess=True, ax =axes[0])\n",
    "axes[0].set_title('Base Model')\n",
    "\n",
    "sns.scatterplot(data = aso_filtered, x ='LOAN_AMOUNT', y = 'LOAN_AMOUNT_explanation',color = \"orange\", ax = axes[1])\n",
    "sns.regplot(data = aso_filtered, x =\"LOAN_AMOUNT\", y = 'LOAN_AMOUNT_explanation', scatter=False, color='blue', line_kws={\"lw\":2},ci =100, lowess=True, ax =axes[1])\n",
    "axes[1].set_title('Opt Model')\n",
    "\n",
    "# Customize and show the plot\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"LOAN_AMOUNT\")\n",
    "    ax.set_ylabel(\"Influence\")\n",
    "    # ax.set_xlim((0,10000))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a03aa9-1f1a-4a4e-809e-b22e438d72aa",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "plot_home_purchase_explanation",
    "resultHeight": 851
   },
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "fig.suptitle(\"HOME PURCHASE LOAN EXPLANATION\")\n",
    "# Plot side-by-side boxplots\n",
    "sns.boxplot(data = all_shap_base, x ='LOAN_PURPOSE_NAME_HOME_PURCHASE', y = 'LOAN_PURPOSE_NAME_HOME_PURCHASE_explanation',\n",
    "            hue='LOAN_PURPOSE_NAME_HOME_PURCHASE', width=0.8, ax=axes[0])\n",
    "axes[0].set_title('Base Model')\n",
    "sns.boxplot(data = all_shap_opt, x ='LOAN_PURPOSE_NAME_HOME_PURCHASE', y = 'LOAN_PURPOSE_NAME_HOME_PURCHASE_explanation',\n",
    "            hue='LOAN_PURPOSE_NAME_HOME_PURCHASE', width=0.4, ax = axes[1])\n",
    "axes[1].set_title('Opt Model')\n",
    "\n",
    "# Customize and show the plot\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Home PURCHASE Loan (1 = True)\")\n",
    "    ax.set_ylabel(\"Influence\")\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea7aad-4e48-4666-a48c-ddc39331cb1f",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "plot_home_imrprovement_explanation"
   },
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "fig.suptitle(\"HOME IMPROVEMENT LOAN EXPLANATION\")\n",
    "# Plot side-by-side boxplots\n",
    "sns.boxplot(data = all_shap_base, x ='LOAN_PURPOSE_NAME_HOME_IMPROVEMENT', y = 'LOAN_PURPOSE_NAME_HOME_IMPROVEMENT_explanation',\n",
    "            hue='LOAN_PURPOSE_NAME_HOME_IMPROVEMENT', width=0.8, ax=axes[0])\n",
    "axes[0].set_title('Base Model')\n",
    "sns.boxplot(data = all_shap_opt, x ='LOAN_PURPOSE_NAME_HOME_IMPROVEMENT', y = 'LOAN_PURPOSE_NAME_HOME_IMPROVEMENT_explanation',\n",
    "            hue='LOAN_PURPOSE_NAME_HOME_IMPROVEMENT', width=0.4, ax = axes[1])\n",
    "axes[1].set_title('Opt Model')\n",
    "\n",
    "# Customize and show the plot\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Home Improvement Loan (1 = True)\")\n",
    "    ax.set_ylabel(\"Influence\")\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7a9ccc-e785-4a82-b9e9-97fd44d5acf2",
   "metadata": {
    "collapsed": false,
    "name": "md_Monitoring_section",
    "resultHeight": 74
   },
   "source": "# Model Monitoring setup\n\n### Prepare Data for Model Monitoring\n\nSave training and test datasets as permanent Snowflake tables to enable model monitoring and drift detection.\n\n```python\ntrain.write.save_as_table(f\"DEMO_MORTGAGE_LENDING_TRAIN_{VERSION_NUM}\", mode=\"overwrite\")\ntest.write.save_as_table(f\"DEMO_MORTGAGE_LENDING_TEST_{VERSION_NUM}\", mode=\"overwrite\")\n```\n\n**What this does:**\n\n**Persists Snowpark DataFrames as tables:**\n- `train` → `DEMO_MORTGAGE_LENDING_TRAIN_{VERSION_NUM}`\n- `test` → `DEMO_MORTGAGE_LENDING_TEST_{VERSION_NUM}`\n\n**Parameters:**\n- **Table names**: Versioned with `VERSION_NUM` for tracking\n- **`mode=\"overwrite\"`**: Replaces table if it already exists (idempotent)\n\n#### **Why Save These Tables?**\n\n**1. Model Monitoring Requirements**\n- Snowflake Model Monitor needs **reference data** to establish baseline distributions\n- Training data serves as the \"expected\" distribution\n- Test data can be used for initial validation\n\n**2. Drift Detection**\n- Future production data will be compared against these reference tables\n- Detect if feature distributions change over time\n- Alert when model performance degrades\n\n**3. Reproducibility**\n- Permanent record of exact data used for training/testing\n- Version control through `VERSION_NUM`\n- Can recreate monitoring setup at any time\n\n**4. Lineage & Governance**\n- Links models to their training data\n- Audit trail for compliance\n- Enables investigation if issues arise\n\n#### **What Happens Next:**\n\nThese tables will be used to:\n1. ✅ **Create Model Monitor** - Reference for baseline metrics\n2. ✅ **Configure drift detection** - Compare production data against baseline\n3. ✅ **Track performance** - Monitor model accuracy over time\n4. ✅ **Trigger alerts** - Notify when retraining is needed\n\n**Result:** Training and test datasets are now permanently stored and ready for model monitoring configuration."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0751bdd-6c24-4c65-9247-aa90ebc1d376",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "create_table_from_test_data",
    "resultHeight": 0,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "train.write.save_as_table(f\"DEMO_MORTGAGE_LENDING_TRAIN_{VERSION_NUM}\", mode=\"overwrite\")\n",
    "test.write.save_as_table(f\"DEMO_MORTGAGE_LENDING_TEST_{VERSION_NUM}\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabdf2be-87f8-4556-aa42-22e4a70515e1",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "create_stage",
    "resultHeight": 111
   },
   "outputs": [],
   "source": [
    "session.sql(\"CREATE stage IF NOT EXISTS ML_STAGE\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2defb24-20a9-451f-ba38-2bd42aa5b320",
   "metadata": {
    "name": "md_define_sproc",
    "collapsed": false
   },
   "source": "### Define and Register Stored Procedure for Model Inference\n\nCreate a reusable stored procedure (SPROC) that retrieves a model from Model Registry and runs batch inference on any specified table.\n\n---\n\n#### **Stored Procedure Function:**\n\n```python\ndef demo_inference_sproc(session: snowpark.Session, table_name: str, modelname: str, modelversion: str) -> str:\n```\n\n**Parameters:**\n- `session`: Snowpark session (automatically provided by Snowflake)\n- `table_name`: Name of the table containing data for inference\n- `modelname`: Model name in Model Registry\n- `modelversion`: Specific model version to use\n\n**Returns:** `\"Success\"` message upon completion\n\n---\n\n#### **What the SPROC Does:**\n\n**1. Retrieve Model from Registry**\n```python\nreg = Registry(session=session)\nm = reg.get_model(model_name)\nmv = m.version(modelversion)\n```\nFetches the specified model version from Snowflake Model Registry\n\n**2. Load Input Data**\n```python\ndf = session.table(input_table_name)\n```\nReads the target table as a Snowpark DataFrame\n\n**3. Run Batch Inference**\n```python\nresults = mv.run(df, function_name=\"predict\").select(\"LOAN_ID\", '\"output_feature_0\"').withColumnRenamed('\"output_feature_0\"', pred_col)\n```\n- Calls the model's `predict` function\n- Selects loan ID and prediction\n- Renames output to versioned column name (e.g., `XGB_BASE_PREDICTION`)\n\n**4. Join Predictions with Original Data**\n```python\nfinal = df.join(results, on=\"LOAN_ID\", how=\"full\")\n```\nCombines original features with predictions\n\n**5. Write Results Back**\n```python\nfinal.write.save_as_table(table_name, mode='overwrite', enable_schema_evolution=True)\n```\n- Overwrites original table with predictions added\n- `enable_schema_evolution=True`: Automatically adds new prediction column\n\n---\n\n#### **Register the SPROC:**\n\n```python\nsession.sproc.register(\n    func=demo_inference_sproc,\n    name=\"model_inference_sproc\",\n    replace=True,\n    is_permanent=True,\n    stage_location=\"@ML_STAGE\",\n    packages=['joblib', 'snowflake-snowpark-python', 'snowflake-ml-python'],\n    return_type=StringType()\n)\n```\n\n**Configuration:**\n- **`name`**: `model_inference_sproc` - How to call it in Snowflake\n- **`replace=True`**: Overwrites if already exists (idempotent)\n- **`is_permanent=True`**: Persists beyond session (survives restarts)\n- **`stage_location`**: `@ML_STAGE` - Where Python packages are stored\n- **`packages`**: Required dependencies for model execution\n- **`return_type`**: Returns a string\n\n---\n\n#### **🎯 Why Use a Stored Procedure?**\n\n**1. Production Deployment**\n- ✅ **Scalable**: Runs on Snowflake compute (no client resources)\n- ✅ **Scheduled**: Can be called by Snowflake Tasks for automation\n- ✅ **Centralized**: Logic lives in Snowflake, not external scripts\n\n**2. Reusability**\n- ✅ **Parameterized**: Works with any table, model, or version\n- ✅ **Versioned**: Can switch models without code changes\n- ✅ **Shareable**: Available to all users with permissions\n\n**3. Model Registry Integration**\n- ✅ **Always uses latest**: Automatically fetches model from registry\n- ✅ **Version control**: Specify exact version for reproducibility\n- ✅ **Lineage tracking**: Maintains connection between models and predictions\n\n---\n\n#### **How to Use:**\n\n```sql\n-- Call the SPROC to run inference\nCALL model_inference_sproc(\n    'DEMO_MORTGAGE_LENDING_TEST_0',  -- table_name\n    'MORTGAGE_LENDING_MLOPS_0',      -- modelname\n    'XGB_BASE'                       -- modelversion\n);\n```\n\n**Result:** Table is updated with a new prediction column (e.g., `XGB_BASE_PREDICTION`)\n\n---\n\n#### **Next Steps:**\n\nAfter registration, this SPROC can be:\n- ✅ Called manually for ad-hoc inference\n- ✅ Scheduled with Snowflake Tasks for batch processing\n- ✅ Integrated into data pipelines\n- ✅ Used for A/B testing different model versions\n\n---\n\n**This SPROC provides a production-ready, scalable way to deploy models from Model Registry for batch inference!**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b2c090-5cc8-4847-982a-fb9b5e427616",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "define_sproc",
    "resultHeight": 495
   },
   "outputs": [],
   "source": [
    "from snowflake import snowpark\n",
    "\n",
    "def demo_inference_sproc(session: snowpark.Session, table_name: str, modelname: str, modelversion: str) -> str:\n",
    "\n",
    "    reg = Registry(session=session)\n",
    "    m = reg.get_model(model_name)  # Fetch the model using the registry\n",
    "    mv = m.version(modelversion)\n",
    "    \n",
    "    input_table_name=table_name\n",
    "    pred_col = f'{modelversion}_PREDICTION'\n",
    "\n",
    "    # Read the input table to a dataframe\n",
    "    df = session.table(input_table_name)\n",
    "    results = mv.run(df, function_name=\"predict\").select(\"LOAN_ID\",'\"output_feature_0\"').withColumnRenamed('\"output_feature_0\"', pred_col)\n",
    "    # 'results' is the output DataFrame with predictions\n",
    "\n",
    "    final = df.join(results, on=\"LOAN_ID\", how=\"full\")\n",
    "    # Write results back to Snowflake table\n",
    "    final.write.save_as_table(table_name, mode='overwrite',enable_schema_evolution=True)\n",
    "\n",
    "    return \"Success\"\n",
    "\n",
    "# Register the stored procedure\n",
    "session.sproc.register(\n",
    "    func=demo_inference_sproc,\n",
    "    name=\"model_inference_sproc\",\n",
    "    replace=True,\n",
    "    is_permanent=True,\n",
    "    stage_location=\"@ML_STAGE\",\n",
    "    packages=['joblib', 'snowflake-snowpark-python', 'snowflake-ml-python'],\n",
    "    return_type=StringType()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45031a-917e-4f6d-a2e4-068879791819",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "gb_base_train_inference",
    "resultHeight": 111
   },
   "outputs": [],
   "source": [
    "CALL model_inference_sproc('DEMO_MORTGAGE_LENDING_TRAIN_{{VERSION_NUM}}','{{model_name}}', '{{base_version_name}}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d18ea05-7d29-43a3-9baa-52509f3bb15e",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "gb_base_test_inference",
    "resultHeight": 111
   },
   "outputs": [],
   "source": [
    "CALL model_inference_sproc('DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}','{{model_name}}', '{{base_version_name}}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2550b-46c7-4eb7-adaa-64c345711b1e",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "gb_opt_train_inference",
    "resultHeight": 111
   },
   "outputs": [],
   "source": [
    "CALL model_inference_sproc('DEMO_MORTGAGE_LENDING_TRAIN_{{VERSION_NUM}}','{{model_name}}', '{{optimized_version_name}}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8245f482-19e9-4961-9cb2-801bf5948d52",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "gb_opt_test_inference",
    "resultHeight": 111
   },
   "outputs": [],
   "source": [
    "CALL model_inference_sproc('DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}','{{model_name}}', '{{optimized_version_name}}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf533f01-6ce5-4fdc-8c3e-20aa48eb947f",
   "metadata": {
    "name": "md_sproc_calls",
    "collapsed": false
   },
   "source": "### Run Batch Inference: Both Models on Train & Test Data\n\nExecute the stored procedure four times to generate predictions from both baseline and optimized models on training and test datasets.\n\n---\n\n#### **Execution Sequence:**\n\n**1. Baseline Model → Training Data**\n```sql\nCALL model_inference_sproc('DEMO_MORTGAGE_LENDING_TRAIN_{{VERSION_NUM}}','{{model_name}}', '{{base_version_name}}');\n```\nAdds `XGB_BASE_PREDICTION` column to training table\n\n**2. Baseline Model → Test Data**\n```sql\nCALL model_inference_sproc('DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}','{{model_name}}', '{{base_version_name}}');\n```\nAdds `XGB_BASE_PREDICTION` column to test table\n\n**3. Optimized Model → Training Data**\n```sql\nCALL model_inference_sproc('DEMO_MORTGAGE_LENDING_TRAIN_{{VERSION_NUM}}','{{model_name}}', '{{optimized_version_name}}');\n```\nAdds `XGB_OPTIMIZED_PREDICTION` column to training table\n\n**4. Optimized Model → Test Data**\n```sql\nCALL model_inference_sproc('DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}','{{model_name}}', '{{optimized_version_name}}');\n```\nAdds `XGB_OPTIMIZED_PREDICTION` column to test table\n\n\nCALL model_inference_sproc('DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}','{{model_name}}', '{{base_version_name}}');ns |\n|-------|---------------------|------------------|\n| Baseline (Overfit) | ✅ | ✅ |\n| Optimized (HPO) | ✅ | ✅ |\n\n**Enables:**\n1. ✅ **Performance comparison** - Calculate metrics for both models side-by-side\n2. ✅ **Overfitting detection** - Compare train vs test performance for each model\n3. ✅ **Model monitoring setup** - Reference data with predictions for drift detection\n4. ✅ **A/B testing** - Both predictions available for evaluation\n\n\n#### **📈 Expected Outcomes:**\n\n**Baseline Model (XGB_BASE):**\n- High training accuracy (overfitting)\n- Lower test accuracy (poor generalization)\n- Large train/test performance gap\n\n**Optimized Model (XGB_OPTIMIZED):**\n- Modest training accuracy (not memorizing)\n- Strong test accuracy (good generalization)\n- Small train/test performance gap\n\n#### **Next Steps:**\n\nWith predictions generated, you can now:\n- 📊 Calculate and compare F1, precision, recall metrics\n- 🎨 Create confusion matrices and ROC curves\n- 📉 Visualize train/test performance differences\n- 🔍 Set up Model Monitors for both model versions\n- ✅ Demonstrate HPO success with concrete metrics\n\n---\n\n**Result:** Complete inference results from both models ready for comprehensive performance analysis and monitoring setup."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05048c-a9d1-4ef9-bf39-5333f3fb56cb",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "see_preds",
    "resultHeight": 251
   },
   "outputs": [],
   "source": [
    "select TIMESTAMP, LOAN_ID, INCOME, LOAN_AMOUNT, XGB_BASE_PREDICTION, XGB_OPTIMIZED_PREDICTION, MORTGAGERESPONSE \n",
    "FROM DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}} \n",
    "limit 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416c348e-0e6d-4ee3-88c3-59077b409621",
   "metadata": {
    "name": "model_monitor_markdown",
    "collapsed": false
   },
   "source": "## Now that our models have been deployed and we have run inference - lets set up ML Observability!\n\n## Set Up Model Monitoring\n\nConfigure monitoring to track performance, detect drift, and ensure production reliability.\n\n---\n\n### **Prepare Segmentation Column**\n\nAdd a human-readable `LOAN_PURPOSE` column to enable drill-down analysis by loan type.\n\n---\n\n```sql\nALTER TABLE DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}\nADD COLUMN IF NOT EXISTS LOAN_PURPOSE VARCHAR(50);\n\nUPDATE DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}\nSET LOAN_PURPOSE = CASE\n    WHEN LOAN_PURPOSE_NAME_HOME_IMPROVEMENT = 1 THEN 'HOME_IMPROVEMENT'\n    WHEN LOAN_PURPOSE_NAME_HOME_PURCHASE = 1 THEN 'HOME_PURCHASE'\n    WHEN LOAN_PURPOSE_NAME_REFINANCING = 1 THEN 'REFINANCING'\n    ELSE 'OTHER'\nEND;\n```\n\n**What this does:**\n- Converts one-hot encoded binary flags to categorical labels\n- Enables segmented monitoring (e.g., drift detection by loan type)\n- Improves dashboard readability and root cause analysis\n\n**Result:** Test table now includes `LOAN_PURPOSE` column for segment-based monitoring.\n\n---\n\n**Next:** Create Model Monitors for both baseline and optimized models."
  },
  {
   "cell_type": "code",
   "id": "a16befd0-410b-4777-bbf6-bfa3580c4973",
   "metadata": {
    "language": "sql",
    "name": "create_segment_col_test",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "ALTER TABLE DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}\nADD COLUMN IF NOT EXISTS LOAN_PURPOSE VARCHAR(50);\n\n\nUPDATE DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}\nSET LOAN_PURPOSE = CASE\n    WHEN LOAN_PURPOSE_NAME_HOME_IMPROVEMENT = 1 THEN 'HOME_IMPROVEMENT'\n    WHEN LOAN_PURPOSE_NAME_HOME_PURCHASE = 1 THEN 'HOME_PURCHASE'\n    WHEN LOAN_PURPOSE_NAME_REFINANCING = 1 THEN 'REFINANCING'\n    ELSE 'OTHER'\nEND;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5a191e3-6311-4bd2-a613-5b21a1df9082",
   "metadata": {
    "language": "sql",
    "name": "create_segment_col_train",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "ALTER TABLE DEMO_MORTGAGE_LENDING_TRAIN_{{VERSION_NUM}}\nADD COLUMN IF NOT EXISTS LOAN_PURPOSE VARCHAR(50);\n\n\nUPDATE DEMO_MORTGAGE_LENDING_TRAIN_{{VERSION_NUM}}\nSET LOAN_PURPOSE = CASE\n    WHEN LOAN_PURPOSE_NAME_HOME_IMPROVEMENT = 1 THEN 'HOME_IMPROVEMENT'\n    WHEN LOAN_PURPOSE_NAME_HOME_PURCHASE = 1 THEN 'HOME_PURCHASE'\n    WHEN LOAN_PURPOSE_NAME_REFINANCING = 1 THEN 'REFINANCING'\n    ELSE 'OTHER'\nEND;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c8d51f7b-8e3e-4a30-b7c5-606986f668cd",
   "metadata": {
    "language": "sql",
    "name": "view_loan_purpose_data",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT LOAN_PURPOSE_NAME_HOME_PURCHASE, LOAN_PURPOSE_NAME_HOME_IMPROVEMENT, LOAN_PURPOSE_NAME_REFINANCING, LOAN_PURPOSE FROM DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}} limit 10;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd1956b1-43cd-4d33-80b2-78cf54cbe363",
   "metadata": {
    "name": "md_create_model_monitor",
    "collapsed": false
   },
   "source": "### Create Model Monitor for Baseline Model\n\nSet up a Model Monitor to track performance, detect drift, and monitor data quality for the baseline (overfit) model.\n\n#### **Configuration Parameters:**\n\n**Model Reference:**\n- **`MODEL`**: `MORTGAGE_LENDING_MLOPS_0` - Model from registry\n- **`VERSION`**: `XGB_BASE` - Baseline model version\n- **`FUNCTION`**: `predict` - Model function to monitor\n\n**Data Sources:**\n- **`SOURCE`**: Test table containing inference results (current predictions)\n- **`BASELINE`**: Training table serving as reference distribution (expected behavior)\n\n**Column Mapping:**\n- **`TIMESTAMP_COLUMN`**: `TIMESTAMP` - For time-series drift analysis\n- **`PREDICTION_CLASS_COLUMNS`**: `XGB_BASE_PREDICTION` - Model's predicted values\n- **`ACTUAL_CLASS_COLUMNS`**: `MORTGAGERESPONSE` - Ground truth labels\n- **`ID_COLUMNS`**: `LOAN_ID` - Unique record identifier\n\n**Segmentation:**\n- **`SEGMENT_COLUMNS`**: `LOAN_PURPOSE` - Enable drill-down by loan type\n\n**Compute & Scheduling:**\n- **`WAREHOUSE`**: `APP_WH` - Compute for monitoring queries\n- **`REFRESH_INTERVAL`**: `12 hours` - How often to update metrics\n- **`AGGREGATION_WINDOW`**: `1 day` - Time window for aggregating metrics\n\n---\n\n#### **What This Monitor Tracks:**\n\n**1. Performance Metrics**\n- Accuracy, F1 score, precision, recall over time\n- Compare predictions vs actual labels\n\n**2. Data Drift**\n- Feature distribution changes (vs baseline training data)\n- Detect shifts in INCOME, LOAN_AMOUNT, etc.\n\n**3. Prediction Drift**\n- Changes in prediction distribution\n- Alert if prediction patterns differ from training\n\n**4. Segmented Analysis**\n- Monitor drift separately for:\n  - HOME_PURCHASE loans\n  - REFINANCING loans  \n  - HOME_IMPROVEMENT loans\n\n---\n\n#### **How It Works:**\nEvery 12 hours:\n1. Query SOURCE table (test data with predictions)\n2. Compare against BASELINE table (training data distribution)\n3. Calculate drift metrics for 1-day windows\n4. Store results for querying/alerting."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6be548-47cb-4a91-92ee-a5f42c41e756",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "create_model_monitor_base",
    "resultHeight": 111
   },
   "outputs": [],
   "source": "CREATE OR REPLACE MODEL MONITOR MORTGAGE_LENDING_BASE_MODEL_MONITOR\nWITH\n    MODEL={{model_name}}\n    VERSION={{base_version_name}}\n    FUNCTION=predict\n    SOURCE=DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}\n    BASELINE=DEMO_MORTGAGE_LENDING_TRAIN_{{VERSION_NUM}}\n    TIMESTAMP_COLUMN=TIMESTAMP\n    PREDICTION_CLASS_COLUMNS=(XGB_BASE_PREDICTION)  \n    ACTUAL_CLASS_COLUMNS=(MORTGAGERESPONSE)\n    ID_COLUMNS=(LOAN_ID)\n    SEGMENT_COLUMNS = ('LOAN_PURPOSE')\n    WAREHOUSE={{COMPUTE_WAREHOUSE}}\n    REFRESH_INTERVAL='12 hours'\n    AGGREGATION_WINDOW='1 day';"
  },
  {
   "cell_type": "markdown",
   "id": "116497d5-ac2f-43c9-b783-8d3bbdba36fe",
   "metadata": {
    "name": "md_monitor_opt_model",
    "collapsed": false
   },
   "source": "### Create Model Monitor for Optimized Model\n\nSet up monitoring for the optimized (HPO-tuned) model to track its performance and drift.\n\n**Key difference from baseline monitor:**\n- **`VERSION`**: `XGB_OPTIMIZED` - Monitors the HPO-tuned model\n- **`PREDICTION_CLASS_COLUMNS`**: `XGB_OPTIMIZED_PREDICTION` - Uses optimized model's predictions\n\n**Configuration:** Same data sources, refresh schedule, and segmentation as baseline monitor\n\n**Result:** Both models now have active monitoring for performance comparison and drift detection."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60965976-f17f-42bc-92ae-e43030bba54e",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "create_model_monitor_optimized",
    "resultHeight": 111
   },
   "outputs": [],
   "source": "CREATE OR REPLACE MODEL MONITOR MORTGAGE_LENDING_OPTIMIZED_MODEL_MONITOR\nWITH\n    MODEL={{model_name}}\n    VERSION={{optimized_version_name}}\n    FUNCTION=predict\n    SOURCE=DEMO_MORTGAGE_LENDING_TEST_{{VERSION_NUM}}\n    BASELINE=DEMO_MORTGAGE_LENDING_TRAIN_{{VERSION_NUM}}\n    TIMESTAMP_COLUMN=TIMESTAMP\n    PREDICTION_CLASS_COLUMNS=(XGB_OPTIMIZED_PREDICTION)  \n    ACTUAL_CLASS_COLUMNS=(MORTGAGERESPONSE)\n    ID_COLUMNS=(LOAN_ID)\n    SEGMENT_COLUMNS = ('LOAN_PURPOSE')\n    WAREHOUSE={{COMPUTE_WAREHOUSE}}\n    REFRESH_INTERVAL='12 hours'\n    AGGREGATION_WINDOW='1 day';"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78b0fc2-555d-458b-aa07-7053859539d4",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "generate_model_registry_link",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Click the generated link to view your model in the model regsitry and check out the model monitors!\n",
    "st.write(f'https://app.snowflake.com/{org_name}/{account_name}/#/data/databases/{DB}/schemas/{SCHEMA}/model/{model_name.upper()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537fc658-38f4-4c9a-980b-cf40ef61a268",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "compute_prediction_drift",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SELECT * FROM TABLE(MODEL_MONITOR_DRIFT_METRIC(\n",
    "'MORTGAGE_LENDING_BASE_MODEL_MONITOR', -- model monitor to use\n",
    "'DIFFERENCE_OF_MEANS', -- metric for computing drift\n",
    "'XGB_BASE_PREDICTION', -- comlumn to compute drift on\n",
    "'1 DAY',  -- day granularity for drift computation\n",
    "DATEADD(DAY, -90, CURRENT_DATE()), -- end date\n",
    "DATEADD(DAY, -60, CURRENT_DATE()) -- start date\n",
    ")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8f8d88-1ebe-4622-89ca-39bce08473d4",
   "metadata": {
    "collapsed": false,
    "name": "SPCS_MD"
   },
   "source": [
    "# SPCS Deployment setup (OPTIONAL)\n",
    "## This is disabled by default but uncommenting the below code cells will allow a user to \n",
    "\n",
    "- ### Create a new compute pool with 3 XL CPU nodes\n",
    "- ### Deploys a service on top of our existing HPO model version\n",
    "- ### Tests out inference on newly created container service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c416a4d0-a95f-4702-9a61-26b61706eb11",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "define_spcs_vars",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "cp_name = \"MORTGAGE_LENDING_INFERENCE_CP\"\nnum_spcs_nodes = '2'\nspcs_instance_family = 'CPU_X64_L'\nservice_name = 'MORTGAGE_LENDING_PREDICTION_SERVICE'\n\ncurrent_database = session.get_current_database().replace('\"', '')\ncurrent_schema = session.get_current_schema().replace('\"', '')\nextended_service_name = f'{current_database}.{current_schema}.{service_name}'"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f6702-8fb2-4aac-9e54-a8673c064074",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "setup_compute_pool",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.sql(f\"alter compute pool if exists {cp_name} stop all\").collect()\nsession.sql(f\"drop compute pool if exists {cp_name}\").collect()\nsession.sql(f\"create compute pool {cp_name} min_nodes={num_spcs_nodes} max_nodes={num_spcs_nodes} instance_family={spcs_instance_family} auto_resume=True auto_suspend_secs=300\").collect()\nsession.sql(f\"describe compute pool {cp_name}\").show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df47725b-e9e7-4f93-bdea-9db09794bd95",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "spcs_deploy_service"
   },
   "outputs": [],
   "source": "#note this may take up to 5 minutes to run\n\nmv_opt.create_service(\n    service_name=extended_service_name,\n    service_compute_pool=cp_name,\n    ingress_enabled=True,\n    max_instances=int(num_spcs_nodes)\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c4f6d4-b16b-4448-bcf3-4f128ccfbe43",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "see_model_versions_with_services",
    "collapsed": false
   },
   "outputs": [],
   "source": "model_registry.get_model(f\"MORTGAGE_LENDING_MLOPS_{VERSION_NUM}\").show_versions()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb9b1a-0741-4e9d-aaf4-2da26c44ffbd",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "run_SPCS_inference"
   },
   "outputs": [],
   "source": "mv_container = model_registry.get_model(f\"MORTGAGE_LENDING_MLOPS_{VERSION_NUM}\").default\nmv_container.run(test, function_name = \"predict\", service_name = \"MORTGAGE_LENDING_PREDICTION_SERVICE\").rename('\"output_feature_0\"', 'XGB_PREDICTION')"
  },
  {
   "cell_type": "code",
   "id": "b4e97fa5-d50c-40c8-a5a9-6135a4162adf",
   "metadata": {
    "language": "sql",
    "name": "view_endpoints",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SHOW ENDPOINTS IN SERVICE E2E_SNOW_MLOPS_DB.MLOPS_SCHEMA.MORTGAGE_LENDING_PREDICTION_SERVICE",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35388bca-f70f-47db-a3ef-3558dda91502",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "stop_compute_pool",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Stop the service to save costs\n",
    "# session.sql(f\"alter compute pool if exists {cp_name} stop all\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000036",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true,
    "name": "conclusion",
    "resultHeight": 202,
    "tags": []
   },
   "source": [
    "## Conclusion \n",
    "\n",
    "#### 🛠️ Snowflake Feature Store tracks feature definitions and maintains lineage of sources and destinations 🛠️\n",
    "#### 🚀 Snowflake Model Registry gives users a secure and flexible framework to log models, tag candidates for production, and run inference and explainability jobs 🚀\n",
    "#### 📈 ML observability in Snowflake allows users to montior model performance over time and detect model, feature, and concept drift 📈\n",
    "#### 🔮 All models logged in the Model Registry can be accessed for inference, explainability, lineage tracking, visibility and more 🔮\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed66e43-9843-4c08-ab02-99ede0155464",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": []
  }
 ]
}