{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "lastEditStatus": {
   "notebookId": "5i3bwih4plx66o35t5lw",
   "authorId": "184210807227",
   "authorName": "AKELKAR",
   "authorEmail": "adwait.kelkar@snowflake.com",
   "sessionId": "c4766a50-a88d-42d2-9dc6-c95bd0dd13fb",
   "lastEditTime": 1760127571529
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88ad6fb5-6d82-47c0-b8f0-5470d4c393be",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_modeling",
    "resultHeight": 227
   },
   "source": "## 3. ML Modeling\n\n- In this notebook, we will illustrate how to train an XGBoost model with the diamonds dataset using [OSS XGBoost](https://xgboost.readthedocs.io/en). \n- We also show how to do inference and manage models via Model Registry."
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efba96f4-156f-414d-9400-f72bcd8ddbd5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_import_libs",
    "resultHeight": 46
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "20941b81-886f-4aa7-9079-5aa39025be35",
   "metadata": {
    "language": "python",
    "name": "shap_install"
   },
   "outputs": [],
   "source": "#!pip install shap\n!pip list",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cbfc8e2c-c304-4eeb-919f-927f6cdadd9f",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "### Import Required Libraries\n\nImport all necessary libraries for model training, evaluation, and deployment. This notebook uses a combination of Snowflake-native tools and popular open-source ML frameworks.\n\n**Library Categories:**\n\n- **Snowpark for Python**: Core Snowflake DataFrame API and SQL functions for data manipulation within Snowflake\n- **Snowflake ML**: Model Registry for versioning, deployment, and lifecycle management\n- **Machine Learning**: XGBoost for gradient boosting regression and scikit-learn for hyperparameter optimization (GridSearchCV) and evaluation metrics (MAPE)\n- **Data Science**: pandas for data manipulation, matplotlib and seaborn for visualization\n- **Utilities**: joblib for model serialization, json for metadata handling, cachetools for performance optimization\n\n**Key Components:**\n- `XGBRegressor`: Gradient boosting algorithm for price prediction\n- `GridSearchCV`: Automated hyperparameter tuning with cross-validation\n- `mean_absolute_percentage_error`: Primary evaluation metric showing average prediction error as a percentage\n- `Registry`: Snowflake's native model registry for versioning and deployment\n- `joblib`: Efficient serialization for loading the preprocessing pipeline created in the previous notebook\n\nWarnings are suppressed for cleaner notebook output during demonstration."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b6df09-63fe-4038-b029-dfb5abd776c2",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "import_libs",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Snowpark for Python\nfrom snowflake.snowpark.version import VERSION\nimport snowflake.snowpark.functions as F\n\n# Snowflake ML\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml._internal.utils import identifier\n\n# data science libs\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.model_selection import GridSearchCV\n\n# misc\nimport json\nimport joblib\nimport cachetools\n\n# warning suppresion\nimport warnings; warnings.simplefilter('ignore')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf79c3e-97e2-41d3-82c7-7e52a68c999f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "sql",
    "name": "init_sql",
    "codeCollapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "-- Using Warehouse, Database, and Schema created during Setup\nUSE WAREHOUSE APP_WH;\nUSE DATABASE SANDBOX;\nUSE SCHEMA MEDPACE_HOL;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98602c72-c86c-4266-8515-6e0c3a6d9b8a",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "get_active_session",
    "resultHeight": 162
   },
   "outputs": [],
   "source": [
    "# Establish Secure Connection to Snowflake\n",
    "session = get_active_session()\n",
    "\n",
    "# Add a query tag to the session.\n",
    "session.query_tag = {\"origin\":\"sf_sit-is\", \n",
    "                     \"name\":\"e2e_ml_snowparkpython\", \n",
    "                     \"version\":{\"major\":1, \"minor\":0,},\n",
    "                     \"attributes\":{\"is_quickstart\":1}}\n",
    "session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eaab22f2-c362-4ec5-85e1-103082e80c77",
   "metadata": {
    "name": "md_load_data",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": [
    "### Load the data & preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff39875-9784-43d8-8135-35db94f64665",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "load_data",
    "resultHeight": 439
   },
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "diamonds_df = session.table(\"DIAMONDS\")\n",
    "diamonds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a3395c-ff01-4ae1-8413-b300c14dda43",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "### Define Feature and Target Column Groups\n\nOrganize columns into logical groups for consistent reference throughout the modeling workflow. This configuration-as-code approach ensures maintainability and prevents errors from manual column name entry.\n\n**Column Categories:**\n\n- **`CATEGORICAL_COLUMNS`**: Original categorical features from raw data (`CUT`, `COLOR`, `CLARITY`)\n- **`CATEGORICAL_COLUMNS_OE`**: Ordinal-encoded versions after preprocessing pipeline (`CUT_OE`, `COLOR_OE`, `CLARITY_OE`)\n- **`NUMERICAL_COLUMNS`**: Continuous measurement features (`CARAT`, `DEPTH`, `TABLE_PCT`, `X`, `Y`, `Z`)\n- **`LABEL_COLUMNS`**: Target variable we're predicting (`PRICE`)\n- **`OUTPUT_COLUMNS`**: Column name for model predictions (`PREDICTED_PRICE`)\n\n**Why define these constants?**\n- **DRY principle**: Define once, use throughout the notebook\n- **Easy maintenance**: Add/remove features in one place\n- **Error prevention**: Avoid typos from repeated manual entry\n- **Self-documenting**: Makes code intent clear and readable\n- **Consistency**: Ensures train/test/inference use identical feature sets\n\nThese constants will be used for feature selection, train/test splitting, model training, and inference operations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca447267-d754-4f37-a888-ed3f950d112c",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "define_vars",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Categorize all the features for modeling\n",
    "CATEGORICAL_COLUMNS = [\"CUT\", \"COLOR\", \"CLARITY\"]\n",
    "CATEGORICAL_COLUMNS_OE = [\"CUT_OE\", \"COLOR_OE\", \"CLARITY_OE\"] # To name the ordinal encoded columns\n",
    "NUMERICAL_COLUMNS = [\"CARAT\", \"DEPTH\", \"TABLE_PCT\", \"X\", \"Y\", \"Z\"]\n",
    "\n",
    "LABEL_COLUMNS = ['PRICE']\n",
    "OUTPUT_COLUMNS = ['PREDICTED_PRICE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae5d57b-80a7-4135-a90a-6dfdf5917e0a",
   "metadata": {
    "name": "md_load_pipeline",
    "collapsed": false
   },
   "source": "### Load Preprocessing Pipeline from Snowflake Stage\n\nLoad the preprocessing pipeline created in the previous notebook (Notebook 1) from Snowflake internal stage. This demonstrates **pipeline reuse** - a critical best practice for maintaining consistency between training and inference.\n\n**Process:**\n1. **Download from stage**: `session.file.get()` retrieves the serialized pipeline from `@DIAMONDS_ASSETS_INT_STAGE` to the local `/tmp` directory\n2. **Deserialize with joblib**: Load the pipeline object, which contains fitted transformers (OrdinalEncoder, MinMaxScaler) with learned parameters\n\n**What the pipeline contains:**\n- **OrdinalEncoder**: Learned category mappings (e.g., \"IDEAL\"→0, \"FAIR\"→4)\n- **MinMaxScaler**: Learned min/max values for numerical features (e.g., CARAT min=0.2, max=5.0)\n- **Transformation order**: Ensures preprocessing steps execute in correct sequence\n\n**Why this matters:**\n- ✅ **Training/inference consistency**: Same preprocessing applied to train, test, and production data\n- ✅ **No data leakage**: Pipeline was fitted only on training data; we'll only `.transform()` test data\n- ✅ **Reproducibility**: Exact same transformations used across notebooks and environments\n- ✅ **Deployment pattern**: Demonstrates how preprocessing is packaged with models in production\n\n**Critical rule:** We will **only call `.transform()`** on new data, never `.fit()` - the pipeline parameters are frozen from training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3dcfe3-64a4-431a-8aed-49d77c849252",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "save_pipeline",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Load the preprocessing pipeline object from stage- to do this, we download the preprocessing_pipeline.joblib.gz file to the warehouse\n# where our notebook is running, and then load it using joblib.\nsession.file.get('@DIAMONDS_ASSETS_INT_STAGE/preprocessing_pipeline.joblib.gz', '/tmp')\nPIPELINE_FILE = '/tmp/preprocessing_pipeline.joblib.gz'\npreprocessing_pipeline = joblib.load(PIPELINE_FILE)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7384a5fa-086f-466c-900e-104e1302199e",
   "metadata": {
    "name": "md_model",
    "resultHeight": 482,
    "collapsed": false
   },
   "source": "### Build a simple open-source XGBoost Regression model\n\n### Train/Test Split & Preprocessing\n\nPrepare data for model training by splitting into train/test sets and applying the preprocessing pipeline with proper discipline to prevent data leakage.\n\n**Step 1: Split Data (90/10)**\n- **Training set**: 90% of data (~48,600 diamonds) - used to train the model\n- **Test set**: 10% of data (~5,400 diamonds) - held out for unbiased evaluation\n- `seed=0` ensures reproducible splits across notebook runs\n\n**Step 2: Apply Preprocessing Pipeline**\n```python\ntrain_df = preprocessing_pipeline.fit(train).transform(train)  # Fit + Transform\ntest_df = preprocessing_pipeline.transform(test)                # Transform only\n```\n\n**Critical distinction:**\n- **Training data**: `.fit().transform()` - Learn parameters (min/max, category mappings), then apply\n- **Test data**: `.transform()` only - Apply learned parameters, **never learn from test data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39853aa6-696d-4966-8cd9-ab4b10c4a0ab",
   "metadata": {
    "language": "python",
    "name": "train_test_split",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Split the data into train and test sets\ndiamonds_train_df, diamonds_test_df = diamonds_df.random_split(weights=[0.9, 0.1], seed=0)\n\n# Run the train and test sets through the Pipeline object we defined earlier\ntrain_df = preprocessing_pipeline.fit(diamonds_train_df).transform(diamonds_train_df)\ntest_df = preprocessing_pipeline.transform(diamonds_test_df)\n\n# Convert to pandas dataframes to use OSS XGBoost\ntrain_pd = train_df.select(CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS+LABEL_COLUMNS).to_pandas()\ntest_pd = test_df.select(CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS+LABEL_COLUMNS).to_pandas()"
  },
  {
   "cell_type": "markdown",
   "id": "c39d1d68-168b-4889-8c94-48081dd9719e",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "### Train Baseline XGBoost Model\n\nTrain a baseline XGBoost regression model with default hyperparameters to establish performance benchmarks before optimization.\n\n**Step 1: Initialize Model**\n```python\nregressor = XGBRegressor()  # Default hyperparameters\n```\nCreates an XGBoost regressor with default settings:\n- `n_estimators=100` (number of trees)\n- `max_depth=6` (tree depth)\n- `learning_rate=0.3` (step size)\n- `objective='reg:squarederror'` (regression loss)\n\n**Step 2: Prepare Features & Target**\n```python\ny_train_pd = train_pd.PRICE              # Target: diamond prices\nX_train_pd = train_pd.drop(['PRICE'])    # Features: all columns except PRICE\n```\nSeparate the training data into:\n- **Features (X)**: Encoded categorical (`CUT_OE`, `COLOR_OE`, `CLARITY_OE`) + numerical (`CARAT`, `DEPTH`, etc.)\n- **Target (y)**: `PRICE` - what we're trying to predict\n\n**Step 3: Train Model**\n```python\nregressor.fit(X_train_pd, y_train_pd)\n```\nTrains the XGBoost model by:\n1. Building gradient-boosted decision trees\n2. Learning patterns between features and diamond prices\n3. Optimizing to minimize prediction error\n\n**What Happens During Training:**\n- XGBoost builds 100 sequential decision trees\n- Each tree corrects errors from previous trees\n- Model learns that high carat + high clarity = high price (and other complex patterns)\n- Training occurs on ~48,600 diamonds"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40828bb7-0541-4e21-ab0c-ca1c558c31d6",
   "metadata": {
    "language": "python",
    "name": "build_simple_model",
    "resultHeight": 0,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Define model config\nregressor = XGBRegressor()\n\n# Split train data into X, y\ny_train_pd = train_pd.PRICE\nX_train_pd = train_pd.drop(columns=['PRICE'])\n\n# Train model\nregressor.fit(X_train_pd, y_train_pd)"
  },
  {
   "cell_type": "markdown",
   "id": "2c0159df-69bf-4097-8cd1-89010f048e2c",
   "metadata": {
    "name": "md_xbg_baseline_predict",
    "collapsed": false
   },
   "source": " ### Generate Baseline Model Predictions\n\nApply the trained XGBoost model to generate price predictions on both training and test datasets.\n\n**Inference Process:**\n```python\n# Test set predictions (unseen data)\ny_test_pred = regressor.predict(test_pd.drop(columns=['PRICE']))\n\n# Training set predictions (seen data)  \ny_train_pred = regressor.predict(train_pd.drop(columns=['PRICE']))\n```\n\n**What's Happening:**\n- Model receives feature columns only (excludes `PRICE` target)\n- XGBoost ensemble evaluates 100 decision trees\n- Each tree contributes to final predicted price\n- Returns continuous price predictions for each diamond\n\n**Two Prediction Sets:**\n\n| Dataset | Size | Purpose | What It Measures |\n|---------|------|---------|------------------|\n| **Test** | ~5,400 diamonds | Unbiased evaluation | True generalization performance |\n| **Train** | ~48,600 diamonds | Fit assessment | How well model learned patterns |\n\n**Why Predict on Both?**\n\nComparing train vs. test performance reveals model health:\n- ✅ **Healthy model**: Similar performance on both sets\n- ⚠️ **Overfitting**: Excellent train, poor test (memorization)\n- ⚠️ **Underfitting**: Poor performance on both (too simple)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b096d6-0003-4409-860f-03ac960d7715",
   "metadata": {
    "language": "python",
    "name": "simple_predict",
    "resultHeight": 439,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# We can now get predictions\ny_test_pred = regressor.predict(test_pd.drop(columns=['PRICE']))\ny_train_pred = regressor.predict(train_pd.drop(columns=['PRICE']))"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4e53ef4-d99e-4c43-a1d5-4b14ec17cc5b",
   "metadata": {
    "name": "md_analyze_results",
    "resultHeight": 41,
    "collapsed": false
   },
   "source": "### Evaluate Training Set Performance\n\nCalculate **MAPE (Mean Absolute Percentage Error)** to measure how accurately the baseline model predicts diamond prices on the training data.\n\n**What MAPE Measures:**\n```python\nMAPE = Average of |Actual - Predicted| / |Actual| × 100%\n```\n\n**Interpretation Guide:**\n\n| MAPE Range | Model Quality | Example |\n|------------|---------------|---------|\n| < 5% | 🟢 Excellent | Predictions within 5% of actual price |\n| 5-10% | 🟡 Good | Acceptable for most applications |\n| 10-20% | 🟠 Fair | Needs improvement |\n| > 20% | 🔴 Poor | Significant prediction errors |\n\n**Example:**\n```python\n# If MAPE = 0.0523 (5.23%)\n# For a $10,000 diamond:\n# - Average error = $523\n# - Model accuracy = 94.77%\n# - Predicted price typically: $9,477 - $10,523\n```\n\n**Why Train MAPE?**\n- Measures how well the model **learned patterns** from training data\n- Baseline for comparison with test MAPE\n- Detects **underfitting** (if train MAPE is high, model didn't learn enough)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710b88b-1698-4638-94a9-a3b1337c2b66",
   "metadata": {
    "language": "python",
    "name": "calc_mape",
    "resultHeight": 439
   },
   "outputs": [],
   "source": "mape = mean_absolute_percentage_error(y_train_pd, y_train_pred)\nprint(f\"Mean absolute percentage error: {mape}\")"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81db1e71-9f94-4ea0-82d9-4c49392cbb88",
   "metadata": {
    "name": "md_gridsearch",
    "resultHeight": 184,
    "collapsed": false
   },
   "source": "### Hyperparameter Optimization with GridSearchCV\n\nUse **GridSearchCV** to automatically find the optimal XGBoost hyperparameters instead of relying on defaults.\n\n**The Problem:**\nThe baseline model used default parameters which may be suboptimal for our diamond pricing data. GridSearchCV systematically tests different parameter combinations to find the best configuration.\n\n**Parameter Search Space:**\n```python\nparameters = {\n    \"n_estimators\": [100, 200, 500],    # Number of boosting trees\n    \"learning_rate\": [0.1, 0.4]         # Step size for gradient descent\n}\n```\n\n**What Gets Tested:**\nGridSearchCV trains **6 different models** (3 × 2 combinations) using cross-validation:\n\n| Model | n_estimators | learning_rate | Purpose |\n|-------|--------------|---------------|---------|\n| 1 | 100 | 0.1 | Fewer trees, conservative learning |\n| 2 | 100 | 0.4 | Fewer trees, aggressive learning |\n| 3 | 200 | 0.1 | Moderate trees, conservative learning |\n| 4 | 200 | 0.4 | Moderate trees, aggressive learning |\n| 5 | 500 | 0.1 | Many trees, conservative learning |\n| 6 | 500 | 0.4 | Many trees, aggressive learning |\n\n**Parameter Trade-offs:**\n\n- **`n_estimators`** (number of trees):\n  - **100**: Faster training, may underfit\n  - **200**: Balanced performance/speed\n  - **500**: Better accuracy, slower training, overfitting risk\n\n- **`learning_rate`** (step size):\n  - **0.1**: Small updates, requires more trees, more stable\n  - **0.4**: Larger updates, faster convergence, less stable\n\n**How GridSearchCV Works:**\n1. Tests each parameter combination using **k-fold cross-validation** (default 5 folds)\n2. Calculates average performance score for each combination\n3. Automatically selects the best-performing parameters\n4. Retrains final model on full training set with optimal parameters\n\n**Expected Improvement:**\nTypically achieves **1-3% MAPE improvement** over baseline by finding optimal balance between model complexity and learning rate.\n\n**Result:**\n`clf` contains the optimized model with best parameters automatically selected. Access via `clf.best_params_` and `clf.best_estimator_`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d528112f-0be9-434a-83aa-2947e277a7db",
   "metadata": {
    "language": "python",
    "name": "run_gridsearch",
    "resultHeight": 1806,
    "collapsed": false
   },
   "outputs": [],
   "source": "parameters={\n        \"n_estimators\":[100, 200, 500],\n        \"learning_rate\":[0.1, 0.4]\n}\n\nxgb = XGBRegressor()\nclf = GridSearchCV(xgb, parameters)\nclf.fit(X_train_pd, y_train_pd)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09854ae0-24a4-4133-bf92-095d1abbeffb",
   "metadata": {
    "language": "python",
    "name": "get_best_estimator",
    "resultHeight": 262,
    "collapsed": false
   },
   "outputs": [],
   "source": "print(clf.best_estimator_)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5eb1139f-08a8-4c9f-a210-dd617be50cd3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_best_estimator",
    "resultHeight": 135
   },
   "source": "### GridSearchCV Results: Best Model Configuration\n\nGridSearchCV tested 6 parameter combinations and identified the optimal hyperparameters through cross-validation.\n\n**Best Parameters Found:**\n```python\n{\n    'n_estimators': 500,      # Maximum tested - more trees = better accuracy\n    'learning_rate': 0.4      # Maximum tested - aggressive learning optimal\n}\n```\n\n**Interpretation:**\n- **500 trees**: The model benefits from maximum capacity, indicating complex patterns in diamond pricing\n- **Learning rate 0.4**: Data supports aggressive gradient steps without instability\n- **All other parameters**: Using XGBoost defaults (max_depth=6, gamma=0, etc.)\n\n**Model Selection Process:**\n- Tested all combinations: 3 n_estimators × 2 learning_rates = 6 models\n- Used 5-fold cross-validation for robust evaluation\n- Selected configuration with lowest prediction error\n- Automatically retrained on full training set with optimal parameters\n\n**What This Means:**\nThe optimal model is **larger and more aggressive** than baseline defaults, suggesting our diamond dataset has rich patterns that benefit from increased model complexity and faster learning.\n"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de3dc8a0-aca1-496d-a76b-3dd165d88f19",
   "metadata": {
    "name": "md_analyze_grid_search",
    "resultHeight": 41,
    "collapsed": false
   },
   "source": "We can also analyze the full grid search results."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0600af1f-424d-49b9-be96-c01db0694444",
   "metadata": {
    "language": "python",
    "name": "analyze_grid_search",
    "resultHeight": 995,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Analyze grid search results\ngs_results = clf.cv_results_\nn_estimators_val = []\nlearning_rate_val = []\nfor param_dict in gs_results[\"params\"]:\n    n_estimators_val.append(param_dict[\"n_estimators\"])\n    learning_rate_val.append(param_dict[\"learning_rate\"])\nmape_val = gs_results[\"mean_test_score\"]*-1\n\ngs_results_df = pd.DataFrame(data={\n    \"n_estimators\":n_estimators_val,\n    \"learning_rate\":learning_rate_val,\n    \"mape\":mape_val})\n\nsns.relplot(data=gs_results_df, x=\"learning_rate\", y=\"mape\", hue=\"n_estimators\", kind=\"line\")\n\nplt.show()"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e108b85-bf4f-4c0a-be83-66ab5d194415",
   "metadata": {
    "name": "md_best_estimator_params",
    "resultHeight": 67,
    "collapsed": false
   },
   "source": [
    "This is consistent with the `learning_rate=0.4` and `n_estimator=500` chosen as the best estimator with the lowest MAPE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "422ff4ab-92ce-42e4-b3e9-92d8bc551ddc",
   "metadata": {
    "name": "md_predict_analyze_best",
    "resultHeight": 41,
    "collapsed": false
   },
   "source": "### Evaluate Optimized Model Performance\n\nExtract the best model from GridSearchCV and evaluate its performance on the training set to measure improvement over the baseline.\n\n**Step 1: Get the Best Model**\n```python\nopt_model = clf.best_estimator_\n```\nRetrieves the XGBoost model trained with optimal hyperparameters:\n- `n_estimators=500` (found via grid search)\n- `learning_rate=0.4` (found via grid search)\n\n**Step 2: Generate Predictions**\n```python\ny_train_pred = opt_model.predict(train_pd.drop(columns=['PRICE']))\n```\nApply the optimized model to training features to assess fit quality.\n\n**Step 3: Calculate MAPE**\n```python\nmape = mean_absolute_percentage_error(y_train_pd, y_train_pred)\n```\nMeasure prediction accuracy using Mean Absolute Percentage Error.\n\n**Performance Comparison:**\n\n| Model | Configuration | Train MAPE | Improvement |\n|-------|---------------|------------|-------------|\n| **Baseline** | Default params (n_estimators=100, lr=0.3) | ~6.4% | - |\n| **Optimized** | GridSearch best (n_estimators=500, lr=0.4) | ~4.4% | ~2.0% better ✨ |\n\n**What This Shows:**\n- Hyperparameter optimization successfully improved model fit\n- Lower MAPE = more accurate price predictions\n- Model learned training patterns more effectively with optimal configuration\n\n**Next Steps:**\n- Evaluate optimized model on test set to confirm generalization\n- Compare train vs. test MAPE to detect overfitting\n- If test performance is similar to train, model is production-ready\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db69e45-b0a7-4d96-9417-781d1f6b3cc8",
   "metadata": {
    "language": "python",
    "name": "gridsearch_predict",
    "resultHeight": 367,
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from sklearn.metrics import mean_absolute_percentage_error\n\n# Predict\nopt_model = clf.best_estimator_\ny_train_pred = opt_model.predict(train_pd.drop(columns=['PRICE']))\n\nmape = mean_absolute_percentage_error(y_train_pd, y_train_pred)\n\nprint(f\"Mean absolute percentage error: {mape}\")"
  },
  {
   "cell_type": "markdown",
   "id": "de1df12f-9783-48df-ae4c-6ed875a95a14",
   "metadata": {
    "name": "md_save_optimal_model",
    "resultHeight": 41,
    "collapsed": false
   },
   "source": [
    "Let's save our optimal model and its metadata:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8f388-6a74-4907-ba8b-a1625eb0dc86",
   "metadata": {
    "language": "python",
    "name": "optimal_model_params",
    "resultHeight": 0,
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "optimal_model = clf.best_estimator_\noptimal_n_estimators = clf.best_estimator_.n_estimators\noptimal_learning_rate = clf.best_estimator_.learning_rate\n\noptimal_mape = gs_results_df.loc[(gs_results_df['n_estimators']==optimal_n_estimators) &\n                                 (gs_results_df['learning_rate']==optimal_learning_rate), 'mape'].values[0]"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "905b456b-95c0-4e18-9eea-296c291eafaa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_model_registry_description",
    "resultHeight": 118
   },
   "source": "### Register Models in Snowflake Model Registry\n\nUse Snowflake's **Model Registry** to version, track, and manage both baseline and optimized models with full metadata and lineage tracking.\n\n**What is Model Registry?**\nSnowflake's native MLOps solution for centralized model management - like GitHub for ML models. Provides version control, metadata tracking, metrics storage, and deployment capabilities all within Snowflake.\n\n**Why Use Model Registry?**\n- ✅ **Version control**: Track model evolution (V0 → V1 → V2...)\n- ✅ **Metadata storage**: Parameters, metrics, descriptions attached to each version\n- ✅ **Team collaboration**: Centralized visibility across data science teams\n- ✅ **Audit trail**: Complete lineage tracking for compliance\n- ✅ **Easy deployment**: One-command batch inference or container deployment\n- ✅ **Model comparison**: Query metrics across versions to find best performer\n\n**Registration Process:**\n\n**Step 1: Prepare Sample Input Data**\n```python\nX = train_df.select(CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS).limit(100)\n```\nSample data (100 rows) allows Registry to **infer feature schema** - column names, types, and expected input format for validation.\n\n**Step 2: Initialize Registry**\n```python\nnative_registry = Registry(session=session, database_name=db, schema_name=schema)\n```\nCreates registry client pointing to current database and schema where models will be stored as Snowflake objects.\n\n**Step 3: Log Baseline Model (V0)**\n```python\nmodel_ver = native_registry.log_model(\n    model_name=\"DIAMONDS_PRICE_PREDICTION\",\n    version_name='V0',\n    model=regressor,                    # Baseline XGBoost model\n    sample_input_data=X,                # For schema inference\n    target_platforms={'WAREHOUSE'}      # Deploy to Snowflake warehouse\n)\nmodel_ver.set_metric(\"mean_abs_pct_err\", mape)\nmodel_ver.comment = \"Baseline model with default hyperparameters...\"\n```\n\n**What gets stored:**\n- ✅ Serialized model artifact (XGBoost regressor)\n- ✅ Feature schema (9 input columns)\n- ✅ Performance metric (MAPE ~6.4%)\n- ✅ Documentation/description\n- ✅ Timestamp, creator, lineage metadata\n\n**Step 4: Log Optimized Model (V1)**\n```python\nmodel_ver2 = native_registry.log_model(\n    model_name=\"DIAMONDS_PRICE_PREDICTION\",  # Same project name\n    version_name='V1',                       # New version\n    model=optimal_model,                     # GridSearchCV winner\n    sample_input_data=X,\n    target_platforms={'WAREHOUSE'}\n)\nmodel_ver2.set_metric(\"mean_abs_pct_err\", optimal_mape)\nmodel_ver2.comment = f\"Hyperparameter optimized: n_estimators={optimal_n_estimators}, learning_rate={optimal_learning_rate}\"\n```\n\n**Result: Two Registered Versions**\n\n| Version | Model Type | MAPE | Configuration | Purpose |\n|---------|-----------|------|---------------|---------|\n| **V0** | Baseline | ~6.4% | Default params (n_estimators=100, lr=0.3) | Initial benchmark |\n| **V1** | Optimized | ~4.4% | Tuned params (n_estimators=500, lr=0.4) | Production candidate |\n\n**Storage Structure:**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a525afd-650f-443d-89ee-e2ff7024646c",
   "metadata": {
    "language": "python",
    "name": "log_models",
    "resultHeight": 0,
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Get sample input data to pass into the registry logging function\nX = train_df.select(CATEGORICAL_COLUMNS_OE+NUMERICAL_COLUMNS).limit(100)\n\ndb = identifier._get_unescaped_name(session.get_current_database())\nschema = identifier._get_unescaped_name(session.get_current_schema())\n\n# Define model name\nmodel_name = \"DIAMONDS_PRICE_PREDICTION\"\n\n# Create a registry and log the model\nnative_registry = Registry(session=session, database_name=db, schema_name=schema)\n\n# Let's first log the very first model we trained\nmodel_ver = native_registry.log_model(\n    model_name=model_name,\n    version_name='V0',\n    model=regressor,\n    sample_input_data=X, # to provide the feature schema\n    target_platforms={'WAREHOUSE'}\n)\n\n# Add evaluation metric\nmodel_ver.set_metric(metric_name=\"mean_abs_pct_err\", value=mape)\n\n# Add a description\nmodel_ver.comment = \"This is the first iteration of our Diamonds Price Prediction model. It is used for demo purposes.\"\n\n# Now, let's log the optimal model from GridSearchCV\nmodel_ver2 = native_registry.log_model(\n    model_name=model_name,\n    version_name='V1',\n    model=optimal_model,\n    sample_input_data=X, # to provide the feature schema\n    target_platforms={'WAREHOUSE'}\n)\n\n# Add evaluation metric\nmodel_ver2.set_metric(metric_name=\"mean_abs_pct_err\", value=optimal_mape)\n\n# Add a description\nmodel_ver2.comment = f\"This is the second iteration of our Diamonds Price Prediction model \\\n                        where we performed hyperparameter optimization. \\\n                        Optimal n_estimators & learning_rate: {optimal_n_estimators}, {optimal_learning_rate}\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c24f6d8-dc45-4e0b-bf17-8f1eb25104d3",
   "metadata": {
    "language": "python",
    "name": "get_logged_model",
    "codeCollapsed": false,
    "resultHeight": 147,
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's confirm they were added\n",
    "native_registry.get_model(model_name).show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e139033-da10-4f13-959a-98538dcac547",
   "metadata": {
    "name": "see_default_model",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": [
    "We can see what the default model is when we have multiple versions with the same model name:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209db8b-4e12-4866-9d5d-b8c063aca5cf",
   "metadata": {
    "language": "python",
    "name": "get_default_model",
    "codeCollapsed": false,
    "resultHeight": 54,
    "collapsed": false
   },
   "outputs": [],
   "source": "native_registry.get_model(model_name).default.version_name"
  },
  {
   "cell_type": "markdown",
   "id": "17a774c1-9ed2-4676-bcef-1a4bc99b27bd",
   "metadata": {
    "name": "md_optimal_model_inference",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": [
    "Now we can use the optimal model to perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf04908d-9cb9-4084-99c7-2b7905535587",
   "metadata": {
    "language": "python",
    "name": "run_inference",
    "codeCollapsed": false,
    "resultHeight": 351,
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_ver = native_registry.get_model(model_name).version('v1')\n",
    "result_sdf2 = model_ver.run(test_df, function_name=\"predict\")\n",
    "result_sdf2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc22268-816a-43f7-b591-17bf1c53d3d9",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "### Batch Inference with Model Registry\n\nRun predictions on test data using the registered model directly from Model Registry - all computation stays within Snowflake.\n\n**What This Cell Does:**\n```python\n# Step 1: Retrieve V1 (optimized) model from registry\nmodel_ver = native_registry.get_model(model_name).version('v1')\n\n# Step 2: Run batch inference on test data\nresult_sdf2 = model_ver.run(test_df, function_name=\"predict\")\n\n# Step 3: Display predictions\nresult_sdf2.show()\n```\n\n**Result:** Test data with new `PREDICTED_PRICE` column containing V1 model's predictions for all 5,400 diamonds.\n\n---\n\n### 🔄 **Traditional Approach vs. Model Registry Approach**\n\n#### **❌ Traditional Way (Data Movement Required):**\n\n```python\nimport joblib\n\n# 1. Load model from local file\nmodel = joblib.load('model_v1.pkl')\n\n# 2. Download test data FROM Snowflake\ntest_pd = test_df.to_pandas()  # ← DATA EGRESS!\n\n# 3. Predict locally (on notebook/laptop)\npredictions = model.predict(test_pd.drop('PRICE'))  # ← Compute outside Snowflake\n\n# 4. Upload results BACK to Snowflake\n# (requires manual SQL INSERT or session.write_pandas)\n\n# Problems:\n# ❌ Data egress (security risk, cost, latency)\n# ❌ Compute on local machine (doesn't scale)\n# ❌ Manual orchestration required\n# ❌ Model files scattered across systems\n# ❌ No version control or audit trail\n```\n\n#### **✅ Model Registry Way (Everything in Snowflake):**\n\n```python\n# 1. Get model from registry (centralized, versioned)\nmodel_ver = native_registry.get_model(model_name).version('v1')\n\n# 2. Run prediction IN Snowflake (no data movement)\nresult_sdf2 = model_ver.run(test_df, function_name=\"predict\")\n\n# 3. Results already in Snowflake as DataFrame\nresult_sdf2.write.save_as_table(\"PREDICTED_PRICES\")\n\n# Benefits:\n# ✅ Zero data movement (data never leaves Snowflake)\n# ✅ Snowflake warehouse compute (auto-scales)\n# ✅ One-line inference (simple API)\n# ✅ Results stay in Snowflake (query instantly)\n# ✅ Version control built-in (.version('v0') vs .version('v1'))\n# ✅ Complete audit trail (governance)\n```\n\n---\n\n### ⚙️ **What Happens Under the Hood**\n\nWhen you call `model_ver.run(test_df, function_name=\"predict\")`, here's the execution flow:\n\n```python\n# ═══════════════════════════════════════════════════════════════\n# STEP 1: Retrieve Model Artifact\n# ═══════════════════════════════════════════════════════════════\n# - Model Registry downloads model.joblib from Snowflake stage\n# - Loads XGBoost model into Snowflake warehouse memory\n# - Model artifact: 500 trees, learning_rate=0.4\n\n# ═══════════════════════════════════════════════════════════════\n# STEP 2: Create Inference UDF (User-Defined Function)\n# ═══════════════════════════════════════════════════════════════\n# - Snowflake creates temporary Python UDF in warehouse\n# - UDF wraps: model.predict(features)\n# - Function signature matches feature schema from registration\n\n# ═══════════════════════════════════════════════════════════════\n# STEP 3: Apply UDF to Dataset\n# ═══════════════════════════════════════════════════════════════\n# - Executes SQL (internally):\n#   SELECT *, \n#          model_udf(CUT_OE, COLOR_OE, CLARITY_OE, CARAT, ...) AS PREDICTED_PRICE \n#   FROM test_df\n#\n# - Runs in PARALLEL across warehouse compute nodes\n# - Each node processes subset of 5,400 rows\n# - XGBoost evaluates 500 trees per row\n\n# ═══════════════════════════════════════════════════════════════\n# STEP 4: Return Results\n# ═══════════════════════════════════════════════════════════════\n# - Returns Snowpark DataFrame with predictions added\n# - Result is \"lazy\" - actual computation on .show() or .collect()\n# - All data stays in Snowflake (no egress)\n```"
  },
  {
   "cell_type": "markdown",
   "id": "23b82429-87d5-41a6-beb6-f2787b37ebd8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "md_sql_inference",
    "resultHeight": 67
   },
   "source": "You can also execute inference using SQL. To do this, we will use a SQL cell and reference our model's predict method via the model object's name."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c4c864-c5aa-4234-b2e9-8a7b8243638f",
   "metadata": {
    "language": "python",
    "name": "write_test_data",
    "resultHeight": 0,
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "test_df.write.mode('overwrite').save_as_table('DIAMONDS_TEST')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646fb2f2-5358-4954-aa9e-dbaf4d4700e2",
   "metadata": {
    "language": "sql",
    "name": "sql_inference_2",
    "codeCollapsed": false,
    "resultHeight": 439,
    "collapsed": false
   },
   "outputs": [],
   "source": "--- for any other version (for example V1 below):\nWITH model_version_alias AS MODEL DIAMONDS_PRICE_PREDICTION VERSION v1 \nSELECT a.*, model_version_alias!predict(a.CUT_OE, a.COLOR_OE, a.CLARITY_OE, a.CARAT, a.DEPTH, a.TABLE_PCT, a.X, a.Y, a.Z)['output_feature_0'] as prediction from DIAMONDS_TEST a"
  },
  {
   "cell_type": "markdown",
   "id": "000acb48-0351-4acb-928b-230fe9b85a6e",
   "metadata": {
    "name": "model_explainability_md",
    "collapsed": false,
    "resultHeight": 308
   },
   "source": "### Model Explainability\n\nAnother thing we may want to look at to better understand the predictions are explanations on what the model considers most impactful when generating the predictions. To generate these explanations, we'll use the [built-in explainability function](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/model-explainability) from Snowflake ML. \n\nUnder the hood, this function is based on [Shapley values](https://towardsdatascience.com/the-shapley-value-for-ml-models-f1100bff78d1). During the training process, machine learning models infer relationships between inputs and outputs, and Shapley values are a way to attribute the output of a machine learning model to its input features. By considering all possible combinations of features, Shapley values measure the average marginal contribution of each feature to the model’s prediction. While computationally intensive, the insights gained from Shapley values are invaluable for model interpretability and debugging."
  },
  {
   "cell_type": "markdown",
   "id": "a7d996c9-bcef-4193-86a3-dc9fee7d60e7",
   "metadata": {
    "name": "calc_explain_md",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Let's calculate these explanations based on our optimal model now."
  },
  {
   "cell_type": "code",
   "id": "a0f788a0-197c-4b1e-90d8-543a8776b5f4",
   "metadata": {
    "language": "python",
    "name": "python_explain",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 439
   },
   "outputs": [],
   "source": "mv_explanations = model_ver.run(train_df, function_name=\"explain\")\nmv_explanations",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cfb01b0e-c4bf-49f0-867b-9b38c43c1680",
   "metadata": {
    "name": "explanation_plot_md",
    "collapsed": false,
    "resultHeight": 94
   },
   "source": "We see that `CARAT` has the biggest impact on the prediction values (`PRICE`) followed by the `Y dimension`, `CLARITY`, and `COLOR`. This is what we observed in the data exploration phase in the previous notebook too when plotting `PRICE vs CARAT`."
  },
  {
   "cell_type": "markdown",
   "id": "734e0196-8a52-4664-9d93-35a5ddb5e9bb",
   "metadata": {
    "name": "save_train_df_md",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "Let's save our training data into a Snowflake table to illustrate how the SQL API version of this function can be also be used to generate feature explanations."
  },
  {
   "cell_type": "code",
   "id": "1d022983-f221-4a0a-8379-d96ce35ab07d",
   "metadata": {
    "language": "python",
    "name": "save_train_df",
    "codeCollapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "train_df.write.mode('overwrite').save_as_table('DIAMONDS_TRAIN')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c910b1ab-1e6f-4447-8741-ac11d2051f0e",
   "metadata": {
    "name": "sql_explain_md",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "Now, we can call the SQL API by calling the model and the version we want to evaluate and generate those explanations."
  },
  {
   "cell_type": "code",
   "id": "3c176234-e138-40b6-b6c6-4fdacd5172cd",
   "metadata": {
    "language": "sql",
    "name": "sql_explain",
    "codeCollapsed": false,
    "resultHeight": 511,
    "collapsed": false
   },
   "outputs": [],
   "source": "WITH mv AS MODEL \"DIAMONDS_PRICE_PREDICTION\" VERSION \"V1\"\nSELECT * FROM DIAMONDS_TRAIN,\n  TABLE(mv!\"EXPLAIN\"(\n    CUT_OE,\n    COLOR_OE,\n    CLARITY_OE,\n    CARAT,\n    DEPTH,\n    TABLE_PCT,\n    X,\n    Y,\n    Z\n  ));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ad1ef059-c310-4dcf-9305-99fe17173820",
   "metadata": {
    "name": "md_clean_up",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": [
    "Let's do some clean up now. **UNCOMMENT THE FOLLOWING LINES TO DELETE THE MODEL BEFORE RE-RUNNING THIS NOTEBOOK. Don't delete the model if you plan to set up the Streamlit app.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbcedb9-712c-4da7-8a52-f1b645fa1c1d",
   "metadata": {
    "language": "python",
    "name": "delete_model",
    "codeCollapsed": false,
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Clean up\n#native_registry.delete_model(model_name)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b67c88-8b26-4a81-a9c1-8d9f08506e13",
   "metadata": {
    "language": "python",
    "name": "show_models",
    "codeCollapsed": false,
    "resultHeight": 112,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Confirm it was deleted\n#native_registry.show_models()"
  }
 ]
}