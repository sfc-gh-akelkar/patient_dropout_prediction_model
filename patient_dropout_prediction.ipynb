{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Patient Dropout Prediction Model\n",
        "\n",
        "This notebook builds an XGBoost classifier to predict patient dropout from clinical trials using Python libraries.\n",
        "\n",
        "**Features:**\n",
        "- **Age**: Patient age (0-99)\n",
        "- **Gender**: Patient gender (MALE/FEMALE)\n",
        "- **Target**: PATIENT_DROPPED indicator (1 = dropped out, 0 = completed)\n",
        "\n",
        "**Tech Stack:**\n",
        "- Snowpark for Python (data access)\n",
        "- XGBoost (gradient boosting classifier)\n",
        "- scikit-learn (preprocessing & metrics)\n",
        "- pandas (data manipulation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries and Setup\n",
        "\n",
        "### Why This Tech Stack?\n",
        "\n",
        "**Python with Snowpark:**\n",
        "- Access Snowflake data directly without data movement\n",
        "- Leverage Snowflake's compute power for data operations\n",
        "- Convert seamlessly to pandas for ML operations\n",
        "\n",
        "**XGBoost:**\n",
        "- Industry-standard gradient boosting algorithm\n",
        "- Handles imbalanced datasets (common in dropout scenarios)\n",
        "- Provides feature importance for clinical interpretability\n",
        "- No feature scaling required (works with raw age values)\n",
        "- Proven performance in healthcare applications\n",
        "\n",
        "**scikit-learn:**\n",
        "- Standard library for preprocessing and evaluation\n",
        "- Extensive metrics for model assessment\n",
        "- Compatible with XGBoost workflow\n",
        "\n",
        "**Visualization Libraries:**\n",
        "- matplotlib/seaborn for clear, publication-ready charts\n",
        "- Essential for communicating findings to clinical stakeholders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "### Import Required Libraries\n",
        "\n",
        "# Snowpark for Python\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "import snowflake.snowpark.functions as F\n",
        "\n",
        "# Data science libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Scikit-learn for preprocessing and metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, \n",
        "    precision_score, \n",
        "    recall_score, \n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    roc_auc_score,\n",
        "    roc_curve\n",
        ")\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Misc\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Establish Snowflake session\n",
        "session = get_active_session()\n",
        "\n",
        "# Add query tag for tracking\n",
        "session.query_tag = {\n",
        "    \"origin\": \"patient_dropout_ml\",\n",
        "    \"model\": \"xgboost_classifier\",\n",
        "    \"version\": {\"major\": 1, \"minor\": 0}\n",
        "}\n",
        "\n",
        "print(\"Session established successfully!\")\n",
        "session\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Training Data\n",
        "\n",
        "Load the patient dropout data from INFORMATICS_SANDBOX.ML_TEST.DOR_ANALYSIS_FF\n",
        "\n",
        "### Why Load Data with Snowpark?\n",
        "\n",
        "**Benefits:**\n",
        "- **No data movement**: Query data directly in Snowflake without extracting to local files\n",
        "- **Security**: Data stays within Snowflake's secure environment\n",
        "- **Scalability**: Leverages Snowflake's warehouse compute for large datasets\n",
        "- **Lazy evaluation**: Operations are pushed down to Snowflake for efficiency\n",
        "\n",
        "**Process:**\n",
        "1. Connect via `get_active_session()` - uses existing notebook session\n",
        "2. Reference table directly - Snowpark creates a DataFrame pointer\n",
        "3. Convert to pandas only when needed for ML operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Load the training data from Snowflake table\n",
        "patient_data_df = session.table(\"INFORMATICS_SANDBOX.ML_TEST.DOR_ANALYSIS_FF\")\n",
        "\n",
        "# Display basic info\n",
        "print(f\"Total records: {patient_data_df.count()}\")\n",
        "patient_data_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Check the schema\n",
        "patient_data_df.describe()\n",
        "\n",
        "# Show column names and types\n",
        "for field in patient_data_df.schema.fields:\n",
        "    print(f\"{field.name}: {field.datatype}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploratory Data Analysis\n",
        "\n",
        "### Why EDA is Critical for Clinical Trials\n",
        "\n",
        "**Understanding the data before modeling:**\n",
        "- **Identify patterns**: Do certain age groups or genders have higher dropout rates?\n",
        "- **Data quality**: Check for missing values, outliers, or data entry errors\n",
        "- **Class imbalance**: Is dropout rare or common? Affects model choice and evaluation\n",
        "- **Feature relationships**: How do age and gender relate to dropout?\n",
        "\n",
        "**Clinical Value:**\n",
        "- Provides actionable insights even before predictive modeling\n",
        "- Helps trial coordinators understand risk factors\n",
        "- Validates that the data matches clinical expectations\n",
        "- Identifies populations that may need additional support\n",
        "\n",
        "**For Stakeholders:**\n",
        "- Visualizations are easier to understand than raw statistics\n",
        "- Age group analysis reveals which demographics need intervention\n",
        "- Gender analysis identifies potential bias or fairness concerns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Convert to pandas for analysis and visualization\n",
        "patient_pd = patient_data_df.to_pandas()\n",
        "\n",
        "# Display column names (important for debugging)\n",
        "print(\"Column names in the data:\")\n",
        "print(patient_pd.columns.tolist())\n",
        "print(\"\\nData types:\")\n",
        "print(patient_pd.dtypes)\n",
        "print(\"\\nSample data:\")\n",
        "print(patient_pd.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate that required columns exist\n",
        "required_columns = ['AGE', 'GENDER', 'PATIENT_DROPPED']\n",
        "missing_columns = [col for col in required_columns if col not in patient_pd.columns]\n",
        "\n",
        "if missing_columns:\n",
        "    print(f\"⚠️ WARNING: Missing required columns: {missing_columns}\")\n",
        "    print(f\"Available columns: {patient_pd.columns.tolist()}\")\n",
        "    print(\"\\nPlease check your data source. Expected columns:\")\n",
        "    print(\"  - AGE (or age)\")\n",
        "    print(\"  - GENDER (or gender)\")\n",
        "    print(\"  - PATIENT_DROPPED (or patient_dropped)\")\n",
        "else:\n",
        "    print(\"✓ All required columns found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Normalize column names to uppercase for consistency\n",
        "patient_pd.columns = patient_pd.columns.str.upper()\n",
        "\n",
        "# Display the normalized column names\n",
        "print(\"Normalized column names:\")\n",
        "print(patient_pd.columns.tolist())\n",
        "\n",
        "# Check data distribution\n",
        "print(\"\\n=== Dataset Overview ===\")\n",
        "print(f\"Total patients: {len(patient_pd)}\")\n",
        "print(f\"Dropout count: {patient_pd['PATIENT_DROPPED'].sum()}\")\n",
        "print(f\"Dropout percentage: {patient_pd['PATIENT_DROPPED'].mean() * 100:.2f}%\")\n",
        "print(f\"\\nAge statistics:\")\n",
        "print(f\"  Mean age: {patient_pd['AGE'].mean():.2f}\")\n",
        "print(f\"  Min age: {patient_pd['AGE'].min()}\")\n",
        "print(f\"  Max age: {patient_pd['AGE'].max()}\")\n",
        "print(f\"  Std age: {patient_pd['AGE'].std():.2f}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\nMissing values:\")\n",
        "print(patient_pd.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Dropout rate by gender\n",
        "print(\"=== Dropout Rate by Gender ===\")\n",
        "gender_stats = patient_pd.groupby('GENDER').agg({\n",
        "    'PATIENT_DROPPED': ['count', 'sum', 'mean']\n",
        "}).round(4)\n",
        "gender_stats.columns = ['Total_Patients', 'Dropout_Count', 'Dropout_Rate']\n",
        "gender_stats['Dropout_Percentage'] = gender_stats['Dropout_Rate'] * 100\n",
        "print(gender_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Dropout rate by age group\n",
        "print(\"\\n=== Dropout Rate by Age Group ===\")\n",
        "patient_pd['AGE_GROUP'] = pd.cut(\n",
        "    patient_pd['AGE'], \n",
        "    bins=[0, 30, 50, 70, 100],\n",
        "    labels=['18-29', '30-49', '50-69', '70+']\n",
        ")\n",
        "\n",
        "age_stats = patient_pd.groupby('AGE_GROUP').agg({\n",
        "    'PATIENT_DROPPED': ['count', 'sum', 'mean']\n",
        "}).round(4)\n",
        "age_stats.columns = ['Total_Patients', 'Dropout_Count', 'Dropout_Rate']\n",
        "age_stats['Dropout_Percentage'] = age_stats['Dropout_Rate'] * 100\n",
        "print(age_stats)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Dropout rate by age group\n",
        "age_stats['Dropout_Percentage'].plot(kind='bar', ax=axes[0], color='steelblue')\n",
        "axes[0].set_title('Dropout Rate by Age Group')\n",
        "axes[0].set_ylabel('Dropout Percentage (%)')\n",
        "axes[0].set_xlabel('Age Group')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Dropout rate by gender\n",
        "gender_stats['Dropout_Percentage'].plot(kind='bar', ax=axes[1], color='coral')\n",
        "axes[1].set_title('Dropout Rate by Gender')\n",
        "axes[1].set_ylabel('Dropout Percentage (%)')\n",
        "axes[1].set_xlabel('Gender')\n",
        "axes[1].tick_params(axis='x', rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Preprocessing\n",
        "\n",
        "Prepare features for XGBoost model training by encoding categorical variables.\n",
        "\n",
        "### Why Encode Gender as Binary?\n",
        "\n",
        "**Machine Learning Requirement:**\n",
        "- ML models require numerical input - cannot process text like \"MALE\"/\"FEMALE\"\n",
        "- Binary encoding (0/1) is the simplest and most interpretable approach\n",
        "\n",
        "**Why Binary Over Other Encoding Methods:**\n",
        "\n",
        "1. **vs. One-Hot Encoding**: \n",
        "   - Binary uses 1 column instead of 2 (MALE, FEMALE)\n",
        "   - More efficient for tree-based models like XGBoost\n",
        "   - No dummy variable trap\n",
        "\n",
        "2. **vs. Ordinal Encoding**: \n",
        "   - Binary doesn't imply order (MALE ≠ \"greater than\" FEMALE)\n",
        "   - Prevents model from learning false relationships\n",
        "\n",
        "3. **vs. Label Encoding**: \n",
        "   - Binary is clearer: MALE=1, FEMALE=0\n",
        "   - Easy to interpret feature importance\n",
        "\n",
        "**XGBoost Compatibility:**\n",
        "- Tree-based models handle binary features efficiently\n",
        "- Creates clear splits: \"Is Male?\" → Yes/No\n",
        "\n",
        "### Why Normalize Column Names?\n",
        "\n",
        "- **Consistency**: Prevents KeyError due to case sensitivity\n",
        "- **Reliability**: Works regardless of Snowflake table schema\n",
        "- **Best Practice**: Standard data engineering approach\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Define feature columns and target\n",
        "FEATURE_COLUMNS = ['AGE', 'GENDER']\n",
        "TARGET_COLUMN = 'PATIENT_DROPPED'\n",
        "\n",
        "# Create a clean dataframe with only required columns\n",
        "df_clean = patient_pd[FEATURE_COLUMNS + [TARGET_COLUMN]].copy()\n",
        "\n",
        "# Handle case variations in gender\n",
        "df_clean['GENDER'] = df_clean['GENDER'].str.upper()\n",
        "\n",
        "# Encode gender: MALE = 1, FEMALE = 0\n",
        "df_clean['GENDER_ENCODED'] = (df_clean['GENDER'] == 'MALE').astype(int)\n",
        "\n",
        "# Drop the original gender column\n",
        "df_clean = df_clean.drop('GENDER', axis=1)\n",
        "\n",
        "# Remove AGE_GROUP if it exists (was created for EDA only)\n",
        "if 'AGE_GROUP' in df_clean.columns:\n",
        "    df_clean = df_clean.drop('AGE_GROUP', axis=1)\n",
        "\n",
        "print(\"Preprocessed data shape:\", df_clean.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df_clean.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Verify no missing values and data types\n",
        "print(\"Data Info:\")\n",
        "print(df_clean.info())\n",
        "print(\"\\nData Statistics:\")\n",
        "print(df_clean.describe())\n",
        "print(\"\\nClass distribution:\")\n",
        "print(df_clean[TARGET_COLUMN].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train/Test Split\n",
        "\n",
        "Split the data into training and testing sets following best practices.\n",
        "\n",
        "### Why 80/20 Split?\n",
        "\n",
        "**Industry Standard:**\n",
        "- **80% training**: Provides sufficient data for model to learn patterns\n",
        "- **20% testing**: Large enough for reliable performance estimates\n",
        "- Common practice in healthcare ML applications\n",
        "\n",
        "**Alternative Split Ratios:**\n",
        "- 70/30: Use if you have limited data\n",
        "- 90/10: Use if you have very large datasets\n",
        "- 60/20/20: Add validation set for hyperparameter tuning\n",
        "\n",
        "**For Clinical Trials:**\n",
        "- 80/20 balances learning with evaluation\n",
        "- Test set represents ~20% of future patients\n",
        "- Enough data to detect performance issues\n",
        "\n",
        "### Why Stratification is Critical?\n",
        "\n",
        "**What is Stratification:**\n",
        "- Maintains the same dropout rate in both train and test sets\n",
        "- If overall dropout is 25%, both sets will have ~25% dropout\n",
        "\n",
        "**Why It Matters:**\n",
        "- **Prevents bias**: Without it, test set might be easier/harder than reality\n",
        "- **Reliable metrics**: Ensures test performance reflects true model capability\n",
        "- **Clinical validity**: Test set represents the same patient mix as training\n",
        "\n",
        "**Example Without Stratification:**\n",
        "- Training set: 30% dropout (model learns from harder cases)\n",
        "- Test set: 15% dropout (model appears better than it is)\n",
        "- **Result**: Overly optimistic performance estimates\n",
        "\n",
        "**With Stratification:**\n",
        "- Both sets: ~25% dropout\n",
        "- Fair evaluation of model's true capability\n",
        "\n",
        "### Why random_state=42?\n",
        "\n",
        "- **Reproducibility**: Same split every time code runs\n",
        "- **Debugging**: Can investigate specific patients in test set\n",
        "- **Comparison**: Others can replicate results\n",
        "- **42**: Computer science tradition (Hitchhiker's Guide to the Galaxy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Prepare features (X) and target (y)\n",
        "X = df_clean[['AGE', 'GENDER_ENCODED']]\n",
        "y = df_clean[TARGET_COLUMN]\n",
        "\n",
        "# Split into train and test sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2, \n",
        "    random_state=42,\n",
        "    stratify=y  # Maintain class distribution in both sets\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"Test set size: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "print(f\"\\nTraining set dropout rate: {y_train.mean()*100:.2f}%\")\n",
        "print(f\"Test set dropout rate: {y_test.mean()*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train XGBoost Model\n",
        "\n",
        "Train an XGBoost classifier following the pattern from MEDPACE_ML_HOL notebooks.\n",
        "\n",
        "### Why XGBoost for Clinical Trial Dropout Prediction?\n",
        "\n",
        "**Technical Advantages:**\n",
        "\n",
        "1. **Gradient Boosting:**\n",
        "   - Builds trees sequentially, each correcting previous errors\n",
        "   - More accurate than single decision trees\n",
        "   - Better than random forests for tabular data\n",
        "\n",
        "2. **Handles Imbalanced Data:**\n",
        "   - Clinical trial dropout is often imbalanced (more completions than dropouts)\n",
        "   - XGBoost's regularization prevents overfitting to majority class\n",
        "   - Can use `scale_pos_weight` if needed\n",
        "\n",
        "3. **No Feature Scaling Required:**\n",
        "   - Works directly with raw age values (18-99)\n",
        "   - Unlike logistic regression or neural networks\n",
        "   - Simplifies preprocessing pipeline\n",
        "\n",
        "4. **Feature Importance:**\n",
        "   - Tells us which features drive predictions\n",
        "   - Critical for clinical stakeholders: \"Why did the model predict this?\"\n",
        "   - Helps validate model makes sense medically\n",
        "\n",
        "5. **Proven in Healthcare:**\n",
        "   - Used in patient risk prediction, readmission forecasting\n",
        "   - Published in medical journals\n",
        "   - Trusted by healthcare data scientists\n",
        "\n",
        "**Model Parameters Explained:**\n",
        "\n",
        "- **n_estimators=100**: Build 100 sequential trees\n",
        "  - More trees = better accuracy but slower training\n",
        "  - 100 is a good starting point\n",
        "  \n",
        "- **max_depth=6**: Each tree can be 6 levels deep\n",
        "  - Deeper trees capture more complex patterns\n",
        "  - Too deep = overfitting risk\n",
        "  - 6 balances complexity and generalization\n",
        "\n",
        "- **learning_rate=0.1**: How much each tree contributes\n",
        "  - Lower = more conservative, needs more trees\n",
        "  - 0.1 is standard default\n",
        "  - Can reduce to 0.01 with more trees for better performance\n",
        "\n",
        "- **random_state=42**: Reproducible results\n",
        "\n",
        "- **eval_metric='logloss'**: Measures prediction confidence\n",
        "  - Lower is better\n",
        "  - Standard for classification\n",
        "\n",
        "### Why XGBoost vs. Other Algorithms?\n",
        "\n",
        "| Algorithm | Pros | Cons | Use Case |\n",
        "|-----------|------|------|----------|\n",
        "| **XGBoost** ✅ | Accurate, handles imbalance, interpretable | Needs tuning | **Best for this task** |\n",
        "| Logistic Regression | Simple, fast, interpretable | Assumes linearity, needs scaling | Good baseline only |\n",
        "| Random Forest | Good accuracy, parallel training | Less accurate than XGBoost | Alternative choice |\n",
        "| Neural Networks | Can learn complex patterns | Black box, needs lots of data | Overkill for 2 features |\n",
        "| Decision Tree | Very interpretable | Poor accuracy, overfits | Too simple |\n",
        "\n",
        "**For Clinical Trials:**\n",
        "- XGBoost balances accuracy with interpretability\n",
        "- Feature importance helps clinical teams understand risk factors\n",
        "- Industry standard for healthcare ML\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Initialize XGBoost Classifier\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=100,        # Number of trees\n",
        "    max_depth=6,             # Maximum tree depth\n",
        "    learning_rate=0.1,       # Step size shrinkage\n",
        "    random_state=42,\n",
        "    eval_metric='logloss'    # Evaluation metric\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Training XGBoost model...\")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Display feature importance\n",
        "feature_names = ['AGE', 'GENDER_ENCODED']\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': xgb_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(feature_importance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Visualize feature importance\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.barh(feature_importance['Feature'], feature_importance['Importance'], color='skyblue')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.title('XGBoost Feature Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Make Predictions\n",
        "\n",
        "Generate predictions on both training and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Generate predictions on test set\n",
        "y_test_pred = xgb_model.predict(X_test)\n",
        "y_test_pred_proba = xgb_model.predict_proba(X_test)[:, 1]  # Probability of dropout\n",
        "\n",
        "# Generate predictions on training set (to check for overfitting)\n",
        "y_train_pred = xgb_model.predict(X_train)\n",
        "y_train_pred_proba = xgb_model.predict_proba(X_train)[:, 1]\n",
        "\n",
        "print(\"Predictions generated successfully!\")\n",
        "print(f\"Test predictions shape: {y_test_pred.shape}\")\n",
        "print(f\"Training predictions shape: {y_train_pred.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Create a predictions dataframe for test set\n",
        "test_predictions_df = pd.DataFrame({\n",
        "    'AGE': X_test['AGE'].values,\n",
        "    'GENDER_ENCODED': X_test['GENDER_ENCODED'].values,\n",
        "    'Actual_Dropout': y_test.values,\n",
        "    'Predicted_Dropout': y_test_pred,\n",
        "    'Dropout_Probability': y_test_pred_proba\n",
        "})\n",
        "\n",
        "print(\"Sample predictions:\")\n",
        "print(test_predictions_df.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "## 8. Model Evaluation - Test Set Performance\n",
        "\n",
        "Calculate comprehensive evaluation metrics on the test set.\n",
        "\n",
        "### Why These Metrics Matter for Clinical Trials\n",
        "\n",
        "**Accuracy:**\n",
        "- **What**: Percentage of correct predictions (dropouts + completions)\n",
        "- **Clinical Meaning**: Overall reliability of predictions\n",
        "- **Limitation**: Can be misleading if data is imbalanced\n",
        "- **Example**: 85% accuracy means 85 out of 100 predictions are correct\n",
        "\n",
        "**Precision (Positive Predictive Value):**\n",
        "- **What**: Of patients predicted to dropout, how many actually drop out?\n",
        "- **Formula**: True Positives / (True Positives + False Positives)\n",
        "- **Clinical Meaning**: How confident can we be when model says \"will dropout\"?\n",
        "- **Use Case**: Resource allocation - don't waste intervention on false alarms\n",
        "- **Example**: 70% precision = 70% of \"predicted dropouts\" actually dropout\n",
        "\n",
        "**Recall (Sensitivity):**\n",
        "- **What**: Of patients who actually dropout, how many did we catch?\n",
        "- **Formula**: True Positives / (True Positives + False Negatives)\n",
        "- **Clinical Meaning**: Are we missing patients who need intervention?\n",
        "- **Use Case**: Patient safety - catching at-risk patients is critical\n",
        "- **Example**: 80% recall = we catch 8 out of 10 actual dropouts\n",
        "\n",
        "**F1 Score:**\n",
        "- **What**: Harmonic mean of precision and recall\n",
        "- **Formula**: 2 × (Precision × Recall) / (Precision + Recall)\n",
        "- **Clinical Meaning**: Balance between catching dropouts and avoiding false alarms\n",
        "- **Best When**: Need to balance both precision and recall\n",
        "- **Range**: 0 (worst) to 1 (perfect)\n",
        "\n",
        "**ROC AUC:**\n",
        "- **What**: Area Under Receiver Operating Characteristic Curve\n",
        "- **Range**: 0.5 (random guessing) to 1.0 (perfect)\n",
        "- **Clinical Meaning**: Overall model discrimination ability\n",
        "- **Interpretation**:\n",
        "  - 0.9-1.0: Excellent\n",
        "  - 0.8-0.9: Good\n",
        "  - 0.7-0.8: Fair\n",
        "  - 0.6-0.7: Poor\n",
        "  - 0.5-0.6: Fail (barely better than random)\n",
        "\n",
        "### Why Check Training Performance Too?\n",
        "\n",
        "**Overfitting Detection:**\n",
        "- **Training accuracy >> Test accuracy**: Model memorized training data\n",
        "- **Example**: Train 95%, Test 70% = Overfitting\n",
        "- **Solution**: Reduce model complexity, add regularization, get more data\n",
        "\n",
        "**Underfitting Detection:**\n",
        "- **Both low**: Model too simple to learn patterns\n",
        "- **Example**: Train 60%, Test 58% = Underfitting\n",
        "- **Solution**: Increase model complexity, add features\n",
        "\n",
        "**Healthy Model:**\n",
        "- **Similar performance**: Train 85%, Test 82%\n",
        "- **Generalizes well**: Will work on new patients\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Calculate test set metrics\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_precision = precision_score(y_test, y_test_pred)\n",
        "test_recall = recall_score(y_test, y_test_pred)\n",
        "test_f1 = f1_score(y_test, y_test_pred)\n",
        "test_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
        "\n",
        "print(\"=== TEST SET PERFORMANCE ===\")\n",
        "print(f\"Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"Precision: {test_precision:.4f}\")\n",
        "print(f\"Recall:    {test_recall:.4f}\")\n",
        "print(f\"F1 Score:  {test_f1:.4f}\")\n",
        "print(f\"ROC AUC:   {test_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Calculate training set metrics (to check for overfitting)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "train_precision = precision_score(y_train, y_train_pred)\n",
        "train_recall = recall_score(y_train, y_train_pred)\n",
        "train_f1 = f1_score(y_train, y_train_pred)\n",
        "train_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
        "\n",
        "print(\"\\n=== TRAINING SET PERFORMANCE ===\")\n",
        "print(f\"Accuracy:  {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
        "print(f\"Precision: {train_precision:.4f}\")\n",
        "print(f\"Recall:    {train_recall:.4f}\")\n",
        "print(f\"F1 Score:  {train_f1:.4f}\")\n",
        "print(f\"ROC AUC:   {train_auc:.4f}\")\n",
        "\n",
        "# Check for overfitting\n",
        "print(\"\\n=== OVERFITTING CHECK ===\")\n",
        "print(f\"Accuracy difference: {abs(train_accuracy - test_accuracy):.4f}\")\n",
        "if abs(train_accuracy - test_accuracy) < 0.05:\n",
        "    print(\"✓ Model generalizes well (difference < 5%)\")\n",
        "else:\n",
        "    print(\"⚠ Possible overfitting detected\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "print(\"\\n=== CONFUSION MATRIX ===\")\n",
        "print(f\"True Negatives:  {cm[0,0]}\")\n",
        "print(f\"False Positives: {cm[0,1]}\")\n",
        "print(f\"False Negatives: {cm[1,0]}\")\n",
        "print(f\"True Positives:  {cm[1,1]}\")\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['No Dropout', 'Dropout'],\n",
        "            yticklabels=['No Dropout', 'Dropout'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix - Test Set')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Classification Report\n",
        "print(\"\\n=== CLASSIFICATION REPORT ===\")\n",
        "print(classification_report(y_test, y_test_pred, \n",
        "                          target_names=['No Dropout', 'Dropout']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. ROC Curve and AUC\n",
        "\n",
        "Visualize model performance across different classification thresholds.\n",
        "\n",
        "### What is ROC Curve?\n",
        "\n",
        "**ROC = Receiver Operating Characteristic**\n",
        "\n",
        "**What It Shows:**\n",
        "- X-axis: False Positive Rate (incorrectly predicted dropouts)\n",
        "- Y-axis: True Positive Rate (correctly predicted dropouts)\n",
        "- Plots performance at every possible threshold (0.0 to 1.0)\n",
        "\n",
        "**How to Read It:**\n",
        "- **Top-left corner**: Perfect model (100% TPR, 0% FPR)\n",
        "- **Diagonal line**: Random guessing (no better than coin flip)\n",
        "- **Area under curve (AUC)**: Overall performance metric\n",
        "\n",
        "### Why ROC/AUC Matters for Clinical Decisions\n",
        "\n",
        "**Threshold Trade-offs:**\n",
        "\n",
        "By default, models predict \"dropout\" if probability > 0.5. But we can adjust:\n",
        "\n",
        "1. **Lower threshold (e.g., 0.3):**\n",
        "   - Catch more dropouts (higher recall)\n",
        "   - More false alarms (lower precision)\n",
        "   - **Use when**: Missing a dropout is costly\n",
        "   - **Example**: Critical trial, need all-hands intervention\n",
        "\n",
        "2. **Higher threshold (e.g., 0.7):**\n",
        "   - Fewer false alarms (higher precision)\n",
        "   - Miss some dropouts (lower recall)\n",
        "   - **Use when**: Resources are limited\n",
        "   - **Example**: Focus only on highest-risk patients\n",
        "\n",
        "3. **Default threshold (0.5):**\n",
        "   - Balanced approach\n",
        "   - **Use when**: No strong preference either way\n",
        "\n",
        "**Clinical Application:**\n",
        "\n",
        "The ROC curve helps trial coordinators decide:\n",
        "- \"Should we intervene at 30% risk or 70% risk?\"\n",
        "- \"What's the cost of false alarms vs. missed patients?\"\n",
        "- \"How do we balance resources with patient safety?\"\n",
        "\n",
        "**AUC Interpretation:**\n",
        "- **0.9-1.0**: Excellent - Trust the model's risk scores\n",
        "- **0.8-0.9**: Good - Useful for triaging patients\n",
        "- **0.7-0.8**: Fair - Use with caution, needs improvement\n",
        "- **< 0.7**: Poor - Not reliable for clinical decisions\n",
        "\n",
        "### Why Better Than Accuracy Alone?\n",
        "\n",
        "**Example Scenario:**\n",
        "- 90% of patients complete the trial (10% dropout)\n",
        "- A \"dummy model\" that predicts \"complete\" for everyone = 90% accuracy!\n",
        "- But it's useless - catches 0% of dropouts\n",
        "- **AUC for dummy model = 0.5** (random guessing)\n",
        "- **AUC for good model = 0.85** (actually useful)\n",
        "\n",
        "**ROC/AUC captures the full picture:**\n",
        "- Works even with imbalanced data\n",
        "- Shows trade-offs at all thresholds\n",
        "- Single number (AUC) summarizes overall performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_test_pred_proba)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {test_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "## 10. Predict on New Patients\n",
        "\n",
        "Apply the trained model to score new patients for dropout risk.\n",
        "\n",
        "### Why Production Scoring Matters\n",
        "\n",
        "**Moving from Model to Action:**\n",
        "- Training and evaluation are academic exercises\n",
        "- **Production scoring** is where the model creates value\n",
        "- This section shows how the model would be used in real clinical operations\n",
        "\n",
        "**Real-World Workflow:**\n",
        "1. **New patients enroll** in clinical trial\n",
        "2. **Capture demographics** (age, gender)\n",
        "3. **Run model** to get dropout probability\n",
        "4. **Categorize risk** (High/Medium/Low)\n",
        "5. **Trigger interventions** for high-risk patients\n",
        "\n",
        "### Risk Categorization Strategy\n",
        "\n",
        "**Why Risk Categories?**\n",
        "- Clinical teams need actionable categories, not just probabilities\n",
        "- \"70% dropout risk\" → \"High Risk\" is clearer for non-technical staff\n",
        "- Enables standardized intervention protocols\n",
        "\n",
        "**Our Categories:**\n",
        "- **High Risk (≥70%)**: Immediate intervention required\n",
        "  - Assign dedicated coordinator\n",
        "  - Weekly check-ins\n",
        "  - Address barriers proactively\n",
        "  \n",
        "- **Medium Risk (40-69%)**: Enhanced monitoring\n",
        "  - Bi-weekly check-ins\n",
        "  - Watch for warning signs\n",
        "  - Provide extra support if needed\n",
        "  \n",
        "- **Low Risk (<40%)**: Standard protocol\n",
        "  - Regular scheduled visits\n",
        "  - Standard communication\n",
        "  - Routine follow-up\n",
        "\n",
        "**Adjusting Thresholds:**\n",
        "These thresholds (70%, 40%) can be adjusted based on:\n",
        "- Available resources\n",
        "- Trial criticality\n",
        "- Historical dropout costs\n",
        "- Intervention effectiveness\n",
        "\n",
        "### Production Considerations\n",
        "\n",
        "**Data Pipeline:**\n",
        "- New patients → Database → Model → Risk Score → Intervention System\n",
        "- Automated daily/weekly batch scoring\n",
        "- Or real-time scoring at enrollment\n",
        "\n",
        "**Model Monitoring:**\n",
        "- Track prediction distribution over time\n",
        "- Alert if patterns change (model drift)\n",
        "- Regular retraining with new data\n",
        "\n",
        "**Clinical Integration:**\n",
        "- Export predictions to trial management system\n",
        "- Dashboard showing high-risk patients\n",
        "- Automatic alerts to coordinators\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create sample new patients to score\n",
        "new_patients_data = pd.DataFrame({\n",
        "    'AGE': [25, 45, 65, 30, 75, 22, 55, 40, 80, 28],\n",
        "    'GENDER': ['FEMALE', 'MALE', 'FEMALE', 'MALE', 'FEMALE', \n",
        "               'MALE', 'FEMALE', 'MALE', 'MALE', 'FEMALE']\n",
        "})\n",
        "\n",
        "print(\"New patients to score:\")\n",
        "print(new_patients_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Preprocess new patients (same as training data)\n",
        "new_patients_data['GENDER_ENCODED'] = (new_patients_data['GENDER'].str.upper() == 'MALE').astype(int)\n",
        "\n",
        "# Prepare features for prediction\n",
        "X_new = new_patients_data[['AGE', 'GENDER_ENCODED']]\n",
        "\n",
        "print(\"\\nPreprocessed features:\")\n",
        "print(X_new)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Make predictions on new patients\n",
        "new_predictions = xgb_model.predict(X_new)\n",
        "new_predictions_proba = xgb_model.predict_proba(X_new)[:, 1]\n",
        "\n",
        "# Create results dataframe\n",
        "new_patients_results = new_patients_data.copy()\n",
        "new_patients_results['Predicted_Dropout'] = new_predictions\n",
        "new_patients_results['Dropout_Probability'] = new_predictions_proba\n",
        "\n",
        "# Add risk category\n",
        "def categorize_risk(prob):\n",
        "    if prob >= 0.7:\n",
        "        return 'High Risk'\n",
        "    elif prob >= 0.4:\n",
        "        return 'Medium Risk'\n",
        "    else:\n",
        "        return 'Low Risk'\n",
        "\n",
        "new_patients_results['Risk_Category'] = new_patients_results['Dropout_Probability'].apply(categorize_risk)\n",
        "\n",
        "print(\"\\n=== NEW PATIENT PREDICTIONS ===\")\n",
        "print(new_patients_results[['AGE', 'GENDER', 'Dropout_Probability', 'Risk_Category']].sort_values('Dropout_Probability', ascending=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Visualize risk distribution for new patients\n",
        "risk_counts = new_patients_results['Risk_Category'].value_counts()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Risk category distribution\n",
        "risk_counts.plot(kind='bar', ax=axes[0], color=['red', 'orange', 'green'])\n",
        "axes[0].set_title('Risk Category Distribution - New Patients')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_xlabel('Risk Category')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Dropout probability distribution\n",
        "axes[1].hist(new_patients_results['Dropout_Probability'], bins=10, color='steelblue', edgecolor='black')\n",
        "axes[1].set_title('Dropout Probability Distribution')\n",
        "axes[1].set_xlabel('Dropout Probability')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].axvline(0.4, color='orange', linestyle='--', label='Medium Risk Threshold')\n",
        "axes[1].axvline(0.7, color='red', linestyle='--', label='High Risk Threshold')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "## 11. Summary and Next Steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Summary\n",
        "\n",
        "**Data Source:**\n",
        "- Table: INFORMATICS_SANDBOX.ML_TEST.DOR_ANALYSIS_FF\n",
        "- Features: Age (numeric), Gender (categorical)\n",
        "- Target: Patient_Dropped (binary: 1=dropped out, 0=completed)\n",
        "\n",
        "**Model Details:**\n",
        "- Algorithm: XGBoost Classifier\n",
        "- Implementation: Python (xgboost library)\n",
        "- Train/Test Split: 80/20 with stratification\n",
        "\n",
        "**Model Performance:**\n",
        "- Test Accuracy: {printed above}\n",
        "- Test AUC: {printed above}  \n",
        "- Precision, Recall, F1: {printed above}\n",
        "\n",
        "**Workflow:**\n",
        "1. ✅ Load data from Snowflake using Snowpark\n",
        "2. ✅ Exploratory data analysis with visualizations\n",
        "3. ✅ Feature encoding (Gender → binary)\n",
        "4. ✅ Train/test split with stratification\n",
        "5. ✅ XGBoost model training\n",
        "6. ✅ Comprehensive evaluation (accuracy, precision, recall, F1, AUC, confusion matrix, ROC curve)\n",
        "7. ✅ Production scoring on new patients with risk categorization\n",
        "\n",
        "### Potential Improvements\n",
        "\n",
        "1. **Feature Engineering:**\n",
        "   - Add medical history features\n",
        "   - Include trial duration and phase\n",
        "   - Add previous trial participation data\n",
        "   - Create age bins or polynomial features\n",
        "\n",
        "2. **Model Enhancements:**\n",
        "   - Hyperparameter tuning with GridSearchCV or RandomizedSearchCV\n",
        "   - Try ensemble methods (Random Forest, LightGBM)\n",
        "   - Implement SMOTE or class weighting if imbalanced\n",
        "   - Feature selection techniques\n",
        "\n",
        "3. **MLOps:**\n",
        "   - Integrate with Snowflake Model Registry\n",
        "   - Set up model monitoring and drift detection\n",
        "   - Create automated retraining pipeline\n",
        "   - Deploy as Snowflake UDF for real-time scoring\n",
        "\n",
        "4. **Validation:**\n",
        "   - Implement k-fold cross-validation\n",
        "   - Test on multiple clinical trial datasets\n",
        "   - Perform temporal validation (train on old data, test on recent)\n",
        "\n",
        "5. **Interpretability:**\n",
        "   - Add SHAP values for model explainability\n",
        "   - Create feature importance visualizations\n",
        "   - Analyze misclassified cases\n",
        "5. Address class imbalance if present (SMOTE, class weights)\n",
        "6. Create a production deployment pipeline\n",
        "7. Integrate with Snowflake Feature Store and Model Registry\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Model Persistence (Optional)\n",
        "\n",
        "Save the trained XGBoost model for future use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Optional: Save model to file for later use\n",
        "# Uncomment to save the model\n",
        "\n",
        "# import joblib\n",
        "# import os\n",
        "\n",
        "# # Create models directory if it doesn't exist\n",
        "# os.makedirs('/tmp/models', exist_ok=True)\n",
        "\n",
        "# # Save the model\n",
        "# model_path = '/tmp/models/patient_dropout_xgboost.joblib'\n",
        "# joblib.dump(xgb_model, model_path)\n",
        "# print(f\"Model saved to: {model_path}\")\n",
        "\n",
        "# # To load the model later:\n",
        "# # loaded_model = joblib.load(model_path)\n",
        "# # predictions = loaded_model.predict(X_new)\n",
        "\n",
        "print(\"Model training and evaluation complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
